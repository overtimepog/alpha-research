2025-11-16 16:30:17,571 - evolve_agent.controller - INFO - Logging to benchmark/plasa_attention/evolve_agent_output/logs/evolve_agent_20251116_163017.log (file: DEBUG, console: INFO)
2025-11-16 16:30:17,903 - evolve_agent.llm.openai - INFO - Initialized OpenAI LLM with model: openai/gpt-5.1-codex-mini
2025-11-16 16:30:17,903 - evolve_agent.llm.ensemble - INFO - Initialized LLM ensemble with models: openai/gpt-5.1-codex-mini (weight: 1.00)
2025-11-16 16:30:17,929 - evolve_agent.llm.openai - INFO - Initialized OpenAI LLM with model: openai/gpt-5.1-codex-mini
2025-11-16 16:30:17,929 - evolve_agent.llm.ensemble - INFO - Initialized LLM ensemble with models: openai/gpt-5.1-codex-mini (weight: 1.00)
2025-11-16 16:30:17,956 - evolve_agent.reward_model - INFO - Initialized RewardModel with OpenRouter API: https://openrouter.ai/api/v1
2025-11-16 16:30:17,957 - evolve_agent.reward_model - INFO - Model: moonshotai/kimi-k2-thinking, Temperature: 0.3
2025-11-16 16:30:17,957 - evolve_agent.prompt.sampler - INFO - Initialized prompt sampler
2025-11-16 16:30:17,957 - evolve_agent.prompt.sampler - INFO - Initialized prompt sampler
2025-11-16 16:30:17,957 - evolve_agent.prompt.sampler - INFO - Set custom templates: system=evaluator_system_message, user=None
2025-11-16 16:30:17,958 - evolve_agent.database - INFO - Initialized program database with 0 programs
2025-11-16 16:30:17,958 - evolve_agent.evaluator - DEBUG - Added /mnt/c/Users/overtime/Documents/GitHub/alpha-research/benchmark/plasa_attention to Python path for local imports
2025-11-16 16:30:38,752 - datasets - DEBUG - PyTorch version 2.9.1 available.
2025-11-16 16:31:25,157 - evolve_agent.evaluator - INFO - Successfully loaded evaluation function from benchmark/plasa_attention/evaluator.py
2025-11-16 16:31:25,157 - evolve_agent.evaluator - INFO - Initialized evaluator with benchmark/plasa_attention/evaluator.py
2025-11-16 16:31:25,157 - evolve_agent.controller - INFO - Initialized EvolveAgent with benchmark/plasa_attention/initial_program.py and benchmark/plasa_attention/evaluator.py
2025-11-16 16:31:25,158 - asyncio - DEBUG - Using selector: EpollSelector
2025-11-16 16:31:25,158 - evolve_agent.controller - INFO - Adding initial program to database
2025-11-16 16:31:25,209 - torchao - WARNING - Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
2025-11-16 16:32:39,890 - evolve_agent.evaluator - INFO - Evaluated program 20151515-d7c5-43ae-b76a-a001f54ff03f in 74.73s: score=0.0112, perplexity=89.2253, accuracy=0.4952, train_loss=6.3318, val_loss=4.4912
2025-11-16 16:32:39,897 - evolve_agent.reward_model - INFO - Scoring 1 research proposals...
2025-11-16 16:32:39,898 - evolve_agent.reward_model - DEBUG - Scoring attempt 1/6 for: proposal
2025-11-16 16:32:42,733 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d5139479-3872-4d92-824e-cebdd4bab1ae', 'json_data': {'messages': [{'role': 'system', 'content': 'You are an expert reviewer tasked with evaluating the quality of a research proposal.\nYour evaluations must be consistent, objective, and based on clear criteria.'}, {'role': 'user', 'content': 'Carefully evaluate the following research proposal and assign a score from 1 to 10.\n\nEvaluation Criteria:\n- Clarity: Is the proposal well-written and easy to understand?\n- Novelty: Does it introduce new ideas or approaches?\n- Technical Rigor: Is the methodology sound and well-justified?\n- Potential Impact: Could this research make a significant contribution?\n\nScoring Guidelines:\n- Scores 1-3: Poor quality, major flaws\n- Scores 4-5: Below average, significant issues\n- Score 6: Slightly above borderline, acceptable\n- Scores 7-8: Good quality, solid contribution\n- Scores 9-10: Excellent, exceptional contribution\n\nYou MUST respond with valid JSON in this exact format:\n{\n  "score": <integer 1-10>,\n  "explanation": "<your detailed evaluation reasoning>"\n}\n\nResearch Proposal:\nPer-Layer Adaptive Sparse Attention (PLASA) Benchmark - November 2025\n\nObjective\n=========\nOptimize the implementation of Per-Layer Adaptive Sparse Attention (PLASA) to achieve\nthe lowest validation perplexity on a 4-layer transformer language model trained on\nWikiText-2 for 1000 steps.\n\nPLASA uses progressive sparsity scheduling based on layer specialization research:\n- Early layers (0-33%): Dense attention (k=L) for local pattern recognition\n- Middle layers (33-66%): Aggressive sparse (k=L/4) due to functional redundancy\n- Late layers (66-100%): Moderate sparse (k=L/2) for global context consolidation\n\nBackground\n==========\nRecent research (Sep-Nov 2025) has shown that different transformer layers specialize\nin distinct functions:\n\n1. Layer Specialization (arXiv:2510.17469, Oct 2025):\n   - Early layers: Rapid specialization in pattern recognition and memorization\n   - Middle layers: Consolidate in-distribution generalization (but show redundancy)\n   - Late layers: Refine for out-of-distribution reasoning and global context\n\n2. Dynamic Attention Mask (DAM) - Oct 2025:\n   - Per-layer and per-head dynamic sparse attention masks\n   - Context-aware sparsity structures learned from frozen models\n   - Achieves long-sequence modeling without retraining\n\n3. DeepSeek Sparse Attention - Lightning Indexer (Nov 2025):\n   - Fast, lightweight token selection using FP8 precision\n   - Two-stage: approximate indexer → exact attention on top-k\n   - Mathematical formulation:\n     * Index scores: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)\n     * Top-k selection: S_t = TopK_k({ I^(ReLU)_{t,s} })\n     * Sparse attention only on selected tokens\n\nMathematical Formulation\n========================\nThe PLASA implementation must include:\n\n1. Lightning Indexer:\n   - Multi-head indexer queries: q_{t,j}^I ∈ R^{d_I} for j=1..H_I\n   - Shared indexer keys: k_s^I ∈ R^{d_I}\n   - Per-head weights: w_{t,j}\n   - Index score: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)\n\n2. Adaptive Top-K Selector:\n   - Causal masking: token t can only attend to s ≤ t\n   - Per-layer k values from progressive schedule\n   - Top-k selection: S_t = TopK_k({ I_{t,s} })\n\n3. Sparse Attention:\n   - Standard scaled dot-product attention on selected tokens\n   - RoPE (Rotary Position Embeddings) for positional encoding\n   - Attention mask from top-k selection\n\n4. Progressive Sparsity Schedule:\n   For a 4-layer model with sequence length L=128:\n   - Layer 0: k = 128 (100% dense)\n   - Layer 1: k = 32  (25% sparse)\n   - Layer 2: k = 32  (25% sparse)\n   - Layer 3: k = 64  (50% sparse)\n\nArchitecture Specifications\n============================\nFixed architecture for fair comparison:\n- 4 transformer layers (all using PLASA)\n- 128 hidden dimensions\n- 4 attention heads\n- 128 sequence length\n- ~1.5M parameters (including indexer)\n\nTraining Configuration\n======================\n- Dataset: WikiText-2 (2M tokens cached)\n- Training: 1000 steps\n- Batch size: 2\n- Learning rate: 3e-4 (AdamW)\n- Gradient clipping: 1.0\n- Dropout: 0.1\n\nEvaluation Metrics\n==================\nPrimary metric: Validation Perplexity (lower is better)\nScoring: score = 1 / perplexity (higher score is better)\n\nAdditional metrics reported:\n- Validation loss\n- Validation accuracy (next-token prediction)\n- Training loss\n\nBaseline Performance (initial_program.py)\n==========================================\nThe provided initial implementation achieves on cosmopedia-v2:\n- Validation Perplexity: ~72-80 (expected range)\n- Validation Accuracy: ~50-55%\n- Score: ~0.0125-0.0139 (1/perplexity)\n\nDataset: cosmopedia-v2 (HuggingFaceTB/smollm-corpus)\nTokenizer: SmolLM-135M\nThis matches the exact setup used in exp3_plasa_gdn_hybrid.\n\nThis baseline implements the full PLASA algorithm with:\n- Lightning Indexer with 4 heads, 32-dim indexer space\n- Progressive sparsity schedule (PROGRESSIVE_SPARSE)\n- RoPE positional embeddings\n- Efficient top-k selection with causal masking\n\nOptimization Goals\n==================\nPotential improvements to explore:\n1. Indexer architecture: Number of heads, dimensionality, activation functions\n2. Sparsity schedules: Alternative schedules (AGGRESSIVE_MIDDLE, DENSE_TO_SPARSE)\n3. Top-k selection: Dynamic k based on input, learned threshold adaptation\n4. Weight initialization: Better initialization for indexer components\n5. Regularization: Dropout rates, gradient clipping strategies\n6. Efficiency optimizations: Quantization, sparse kernels, fused operations\n\nConstraints\n===========\n- Must use the progressive sparsity principle (different k per layer)\n- Must implement the Lightning Indexer concept (fast token selection)\n- Model architecture fixed (4 layers, 128 dim, 4 heads)\n- Training budget fixed (1000 steps)\n- Must be self-contained in initial_program.py (no external dependencies except PyTorch)\n\nComparison Context\n==================\nThis benchmark is based on Experiment 3 (exp3_plasa_gdn_hybrid) which showed:\n- PLASA with all 4 layers achieved 51.69% accuracy, 73.81 perplexity\n- PLASA outperformed full attention by 18.4% (lower loss)\n- PLASA outperformed uniform sparse attention (Exp1) by 33.9%\n- Training time: 35.5s for 1000 steps (74% faster than hybrids)\n\nThe benchmark tests whether alternative implementations can match or exceed this\nbaseline performance through architectural innovations, better sparsity schedules,\nor optimization techniques.\n\nNotes\n=====\n- Evaluator uses WikiText-2 if available, falls back to synthetic data\n- Results cached to avoid reprocessing\n- Validation evaluated on 100 batches for speed\n- Comparisons should use the same random seed (42) for reproducibility\n- Higher score is better (score = 1/perplexity)\n- Perplexity capped at 10000 to avoid division issues\n\nReferences\n==========\n- DeepSeek Sparse Attention (2025): Lightning Indexer, FP8 quantization\n- Dynamic Attention Mask (GitHub: ResponsibleAILab/DAM, Oct 2025)\n- Layer Specialization (arXiv:2510.17469, Oct 2025)\n- Transformer Layers as Painters (Emergence.ai, Aug 2024-2025)\n- Original PLASA implementation (exp3_plasa_gdn_hybrid, 2025)\n\n'}], 'model': 'moonshotai/kimi-k2-thinking', 'max_tokens': 4096, 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'research_proposal_score', 'schema': {'type': 'object', 'properties': {'score': {'type': 'integer', 'description': 'Numerical score from 1 to 10', 'minimum': 1, 'maximum': 10}, 'explanation': {'type': 'string', 'description': 'Detailed reasoning for the score'}}, 'required': ['score', 'explanation'], 'additionalProperties': False}, 'strict': True}}, 'temperature': 0.3, 'top_p': 0.95}}
2025-11-16 16:32:42,736 - openai._base_client - DEBUG - Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2025-11-16 16:32:42,772 - httpcore.connection - DEBUG - connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=60.0 socket_options=None
2025-11-16 16:32:42,830 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734a9e9be0>
2025-11-16 16:32:42,831 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7574c5d4d3d0> server_hostname='openrouter.ai' timeout=60.0
2025-11-16 16:32:42,872 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734aa17b00>
2025-11-16 16:32:42,873 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-11-16 16:32:42,874 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-11-16 16:32:42,875 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-11-16 16:32:42,875 - httpcore.http11 - DEBUG - send_request_body.complete
2025-11-16 16:32:42,875 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-11-16 16:32:45,248 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 16 Nov 2025 21:32:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Content-Encoding', b'gzip'), (b'Access-Control-Allow-Origin', b'*'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99fa1a456926c9b0-IAD')])
2025-11-16 16:32:45,250 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-16 16:32:45,251 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-11-16 16:33:57,834 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-11-16 16:33:57,835 - httpcore.http11 - DEBUG - response_closed.started
2025-11-16 16:33:57,835 - httpcore.http11 - DEBUG - response_closed.complete
2025-11-16 16:33:57,836 - openai._base_client - DEBUG - HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "200 OK" Headers({'date': 'Sun, 16 Nov 2025 21:32:46 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99fa1a456926c9b0-IAD'})
2025-11-16 16:33:57,836 - openai._base_client - DEBUG - request_id: None
2025-11-16 16:33:57,846 - evolve_agent.reward_model - DEBUG - Successfully parsed JSON score: 4.0
2025-11-16 16:33:57,846 - evolve_agent.reward_model - INFO - Successfully scored '': 4.0/10
2025-11-16 16:33:57,849 - evolve_agent.reward_model - INFO - Completed scoring: 1/1 successful
2025-11-16 16:33:57,850 - evolve_agent.database - DEBUG - Set initial best program to 20151515-d7c5-43ae-b76a-a001f54ff03f
2025-11-16 16:33:57,850 - evolve_agent.database - DEBUG - Added program 20151515-d7c5-43ae-b76a-a001f54ff03f to island 0
2025-11-16 16:33:57,850 - evolve_agent.controller - INFO - Starting evolution from iteration 0 for 50 iterations (total: 50)
2025-11-16 16:33:57,851 - evolve_agent.controller - INFO - Using island-based evolution with 5 islands
2025-11-16 16:33:57,851 - evolve_agent.database - INFO - Island Status:
2025-11-16 16:33:57,851 - evolve_agent.database - INFO -  * Island 0: 1 programs, best=20.1109, avg=20.1109, diversity=0.00, gen=0
2025-11-16 16:33:57,851 - evolve_agent.database - INFO -    Island 1: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0
2025-11-16 16:33:57,852 - evolve_agent.database - INFO -    Island 2: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0
2025-11-16 16:33:57,852 - evolve_agent.database - INFO -    Island 3: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0
2025-11-16 16:33:57,852 - evolve_agent.database - INFO -    Island 4: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0
2025-11-16 16:33:57,853 - evolve_agent.database - DEBUG - Sampled parent 20151515-d7c5-43ae-b76a-a001f54ff03f and 0 inspirations
2025-11-16 16:33:57,855 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-90e93994-4f5f-42a4-9bf1-e721aada2933', 'json_data': {'messages': [{'role': 'system', 'content': 'You are a research advisor tasked with evolving and improving research proposals. \nYour goal is to generate a new research proposal that builds upon the current proposal while addressing its limitations and incorporating insights from successful approaches.\n\nFocus on:\n1. Identifying weaknesses in the current approach based on performance metrics\n2. Proposing novel improvements that could enhance performance\n3. Learning from successful inspirations while maintaining originality\n4. Ensuring the new proposal is technically sound and implementable'}, {'role': 'user', 'content': 'Based on the following information, generate an improved research proposal:\n\n- Current Proposal:\nPer-Layer Adaptive Sparse Attention (PLASA) Benchmark - November 2025\n\nObjective\n=========\nOptimize the implementation of Per-Layer Adaptive Sparse Attention (PLASA) to achieve\nthe lowest validation perplexity on a 4-layer transformer language model trained on\nWikiText-2 for 1000 steps.\n\nPLASA uses progressive sparsity scheduling based on layer specialization research:\n- Early layers (0-33%): Dense attention (k=L) for local pattern recognition\n- Middle layers (33-66%): Aggressive sparse (k=L/4) due to functional redundancy\n- Late layers (66-100%): Moderate sparse (k=L/2) for global context consolidation\n\nBackground\n==========\nRecent research (Sep-Nov 2025) has shown that different transformer layers specialize\nin distinct functions:\n\n1. Layer Specialization (arXiv:2510.17469, Oct 2025):\n   - Early layers: Rapid specialization in pattern recognition and memorization\n   - Middle layers: Consolidate in-distribution generalization (but show redundancy)\n   - Late layers: Refine for out-of-distribution reasoning and global context\n\n2. Dynamic Attention Mask (DAM) - Oct 2025:\n   - Per-layer and per-head dynamic sparse attention masks\n   - Context-aware sparsity structures learned from frozen models\n   - Achieves long-sequence modeling without retraining\n\n3. DeepSeek Sparse Attention - Lightning Indexer (Nov 2025):\n   - Fast, lightweight token selection using FP8 precision\n   - Two-stage: approximate indexer → exact attention on top-k\n   - Mathematical formulation:\n     * Index scores: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)\n     * Top-k selection: S_t = TopK_k({ I^(ReLU)_{t,s} })\n     * Sparse attention only on selected tokens\n\nMathematical Formulation\n========================\nThe PLASA implementation must include:\n\n1. Lightning Indexer:\n   - Multi-head indexer queries: q_{t,j}^I ∈ R^{d_I} for j=1..H_I\n   - Shared indexer keys: k_s^I ∈ R^{d_I}\n   - Per-head weights: w_{t,j}\n   - Index score: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)\n\n2. Adaptive Top-K Selector:\n   - Causal masking: token t can only attend to s ≤ t\n   - Per-layer k values from progressive schedule\n   - Top-k selection: S_t = TopK_k({ I_{t,s} })\n\n3. Sparse Attention:\n   - Standard scaled dot-product attention on selected tokens\n   - RoPE (Rotary Position Embeddings) for positional encoding\n   - Attention mask from top-k selection\n\n4. Progressive Sparsity Schedule:\n   For a 4-layer model with sequence length L=128:\n   - Layer 0: k = 128 (100% dense)\n   - Layer 1: k = 32  (25% sparse)\n   - Layer 2: k = 32  (25% sparse)\n   - Layer 3: k = 64  (50% sparse)\n\nArchitecture Specifications\n============================\nFixed architecture for fair comparison:\n- 4 transformer layers (all using PLASA)\n- 128 hidden dimensions\n- 4 attention heads\n- 128 sequence length\n- ~1.5M parameters (including indexer)\n\nTraining Configuration\n======================\n- Dataset: WikiText-2 (2M tokens cached)\n- Training: 1000 steps\n- Batch size: 2\n- Learning rate: 3e-4 (AdamW)\n- Gradient clipping: 1.0\n- Dropout: 0.1\n\nEvaluation Metrics\n==================\nPrimary metric: Validation Perplexity (lower is better)\nScoring: score = 1 / perplexity (higher score is better)\n\nAdditional metrics reported:\n- Validation loss\n- Validation accuracy (next-token prediction)\n- Training loss\n\nBaseline Performance (initial_program.py)\n==========================================\nThe provided initial implementation achieves on cosmopedia-v2:\n- Validation Perplexity: ~72-80 (expected range)\n- Validation Accuracy: ~50-55%\n- Score: ~0.0125-0.0139 (1/perplexity)\n\nDataset: cosmopedia-v2 (HuggingFaceTB/smollm-corpus)\nTokenizer: SmolLM-135M\nThis matches the exact setup used in exp3_plasa_gdn_hybrid.\n\nThis baseline implements the full PLASA algorithm with:\n- Lightning Indexer with 4 heads, 32-dim indexer space\n- Progressive sparsity schedule (PROGRESSIVE_SPARSE)\n- RoPE positional embeddings\n- Efficient top-k selection with causal masking\n\nOptimization Goals\n==================\nPotential improvements to explore:\n1. Indexer architecture: Number of heads, dimensionality, activation functions\n2. Sparsity schedules: Alternative schedules (AGGRESSIVE_MIDDLE, DENSE_TO_SPARSE)\n3. Top-k selection: Dynamic k based on input, learned threshold adaptation\n4. Weight initialization: Better initialization for indexer components\n5. Regularization: Dropout rates, gradient clipping strategies\n6. Efficiency optimizations: Quantization, sparse kernels, fused operations\n\nConstraints\n===========\n- Must use the progressive sparsity principle (different k per layer)\n- Must implement the Lightning Indexer concept (fast token selection)\n- Model architecture fixed (4 layers, 128 dim, 4 heads)\n- Training budget fixed (1000 steps)\n- Must be self-contained in initial_program.py (no external dependencies except PyTorch)\n\nComparison Context\n==================\nThis benchmark is based on Experiment 3 (exp3_plasa_gdn_hybrid) which showed:\n- PLASA with all 4 layers achieved 51.69% accuracy, 73.81 perplexity\n- PLASA outperformed full attention by 18.4% (lower loss)\n- PLASA outperformed uniform sparse attention (Exp1) by 33.9%\n- Training time: 35.5s for 1000 steps (74% faster than hybrids)\n\nThe benchmark tests whether alternative implementations can match or exceed this\nbaseline performance through architectural innovations, better sparsity schedules,\nor optimization techniques.\n\nNotes\n=====\n- Evaluator uses WikiText-2 if available, falls back to synthetic data\n- Results cached to avoid reprocessing\n- Validation evaluated on 100 batches for speed\n- Comparisons should use the same random seed (42) for reproducibility\n- Higher score is better (score = 1/perplexity)\n- Perplexity capped at 10000 to avoid division issues\n\nReferences\n==========\n- DeepSeek Sparse Attention (2025): Lightning Indexer, FP8 quantization\n- Dynamic Attention Mask (GitHub: ResponsibleAILab/DAM, Oct 2025)\n- Layer Specialization (arXiv:2510.17469, Oct 2025)\n- Transformer Layers as Painters (Emergence.ai, Aug 2024-2025)\n- Original PLASA implementation (exp3_plasa_gdn_hybrid, 2025)\n\n\n- Current Program:\n```python\n"""\nAdaptive Per-Layer Sparse Attention Implementation\n\nThis module implements sparse attention with layer-specific top-k values.\nBased on research showing different layers specialize in different functions:\n- Early layers: Local patterns, short-range dependencies\n- Middle layers: Feature composition, functionally redundant\n- Late layers: Global context consolidation, semantic abstraction\n\nKey Innovation: Each layer has a different sparsity budget (k value) optimized\nfor its functional role in the transformer hierarchy.\n\nReferences:\n- "Learning to Skip the Middle Layers of Transformers" (2025)\n- "Transformer Layers as Painters" - Emergence.ai (2025)\n- DeepSeek-V3.2-Exp Lightning Indexer\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtune.modules import RotaryPositionalEmbeddings\nfrom typing import Optional, Tuple, List, Dict\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass SparsitySchedule(Enum):\n    """Predefined sparsity schedules for different hypotheses"""\n    DENSE_BASELINE = "dense_baseline"\n    UNIFORM_SPARSE = "uniform_sparse"\n    DENSE_TO_SPARSE = "dense_to_sparse"\n    AGGRESSIVE_MIDDLE = "aggressive_middle"\n    PROGRESSIVE_SPARSE = "progressive_sparse"\n    REVERSE_PROGRESSIVE = "reverse_progressive"\n\n\n@dataclass\nclass LayerSparsityConfig:\n    """Configuration for per-layer sparsity"""\n    schedule_name: str\n    layer_k_values: List[int]  # k value for each layer\n    layer_k_ratios: List[float]  # k as fraction of sequence length\n    description: str\n\n    def get_k_for_layer(self, layer_idx: int, seq_len: int) -> int:\n        """Get k value for a specific layer"""\n        if layer_idx >= len(self.layer_k_ratios):\n            # Default to last value if layer index exceeds config\n            ratio = self.layer_k_ratios[-1]\n        else:\n            ratio = self.layer_k_ratios[layer_idx]\n\n        k = int(seq_len * ratio)\n        return max(1, min(k, seq_len))  # Clamp to [1, seq_len]\n\n\ndef create_sparsity_schedule(\n    schedule: SparsitySchedule,\n    n_layers: int,\n    seq_len: int\n) -> LayerSparsityConfig:\n    """\n    Create a sparsity schedule based on predefined patterns\n\n    Args:\n        schedule: Schedule type\n        n_layers: Number of transformer layers\n        seq_len: Sequence length\n\n    Returns:\n        LayerSparsityConfig with per-layer k values\n    """\n    if schedule == SparsitySchedule.DENSE_BASELINE:\n        # All layers dense (no sparsity)\n        ratios = [1.0] * n_layers\n        description = "Baseline: All layers dense (k=L)"\n\n    elif schedule == SparsitySchedule.UNIFORM_SPARSE:\n        # All layers uniform 50% sparsity (Exp2 baseline)\n        ratios = [0.5] * n_layers\n        description = "Uniform: All layers k=L/2 (Exp2 baseline)"\n\n    elif schedule == SparsitySchedule.DENSE_TO_SPARSE:\n        # Conservative: Dense early, gradually sparse\n        # Early (0-33%): Dense (k=L)\n        # Middle (33-66%): Moderate sparse (k=L/2)\n        # Late (66-100%): Light sparse (k=3L/4)\n        ratios = []\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n\n        for i in range(n_layers):\n            if i < early_cutoff:\n                ratios.append(1.0)  # Dense\n            elif i < middle_cutoff:\n                ratios.append(0.5)  # Moderate sparse\n            else:\n                ratios.append(0.75)  # Light sparse\n        description = "Dense-to-Sparse: Early=Dense, Middle=L/2, Late=3L/4"\n\n    elif schedule == SparsitySchedule.AGGRESSIVE_MIDDLE:\n        # Based on redundancy research: Middle layers most sparse\n        # Early: Moderate (k=L/2)\n        # Middle: Aggressive (k=L/4) - most redundant\n        # Late: Moderate (k=L/2)\n        ratios = []\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n\n        for i in range(n_layers):\n            if i < early_cutoff:\n                ratios.append(0.5)  # Moderate\n            elif i < middle_cutoff:\n                ratios.append(0.25)  # Aggressive sparse\n            else:\n                ratios.append(0.5)  # Moderate\n        description = "Aggressive-Middle: Early=L/2, Middle=L/4, Late=L/2"\n\n    elif schedule == SparsitySchedule.PROGRESSIVE_SPARSE:\n        # Original hypothesis: Dense foundation, aggressive middle, moderate late\n        # Early: Dense (k=L)\n        # Middle: Aggressive (k=L/4)\n        # Late: Moderate (k=L/2)\n        ratios = []\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n\n        for i in range(n_layers):\n            if i < early_cutoff:\n                ratios.append(1.0)  # Dense\n            elif i < middle_cutoff:\n                ratios.append(0.25)  # Aggressive sparse\n            else:\n                ratios.append(0.5)  # Moderate\n        description = "Progressive-Sparse: Early=Dense, Middle=L/4, Late=L/2"\n\n    else:\n        raise ValueError(f"Unknown schedule: {schedule}")\n\n    # Compute actual k values\n    k_values = [int(seq_len * ratio) for ratio in ratios]\n\n    return LayerSparsityConfig(\n        schedule_name=schedule.value,\n        layer_k_values=k_values,\n        layer_k_ratios=ratios,\n        description=description\n    )\n\n\nclass LightningIndexer(nn.Module):\n    """\n    Lightning Indexer for DeepSeek Sparse Attention\n\n    Computes index scores I_{t,s} = Σ w_{t,j} · ReLU(q_{t,j} · k_s)\n\n    Args:\n        d_model: Model dimension\n        indexer_heads: Number of indexer heads (H_I)\n        indexer_dim: Dimension of indexer queries/keys (d_I)\n        dropout: Dropout probability\n    """\n    def __init__(\n        self,\n        d_model: int,\n        indexer_heads: int = 4,\n        indexer_dim: int = 64,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.indexer_heads = indexer_heads\n        self.indexer_dim = indexer_dim\n\n        # Indexer query projection: h_t -> {q_{t,j}^I}\n        self.q_proj = nn.Linear(d_model, indexer_heads * indexer_dim, bias=False)\n\n        # Indexer key projection: h_s -> k_s^I\n        self.k_proj = nn.Linear(d_model, indexer_dim, bias=False)\n\n        # Indexer weights: w_{t,j}^I for each head\n        self.w_proj = nn.Linear(d_model, indexer_heads, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """\n        Compute index scores between all pairs of tokens\n\n        Args:\n            x: Input tensor [batch_size, seq_len, d_model]\n\n        Returns:\n            index_scores: Index scores [batch_size, seq_len, seq_len]\n        """\n        batch_size, seq_len, _ = x.shape\n\n        # Compute indexer queries: [batch, seq_len, indexer_heads, indexer_dim]\n        queries = self.q_proj(x).reshape(batch_size, seq_len, self.indexer_heads, self.indexer_dim)\n\n        # Compute indexer keys: [batch, seq_len, indexer_dim]\n        keys = self.k_proj(x)\n\n        # Compute indexer weights: [batch, seq_len, indexer_heads]\n        weights = self.w_proj(x)\n\n        # Compute dot products: q_{t,j} · k_s for all t, s, j\n        dots = torch.einsum(\'bthd,bsd->bths\', queries, keys)\n\n        # Apply ReLU activation\n        activated = F.relu(dots)\n\n        # Weight each head: w_{t,j} · ReLU(q_{t,j} · k_s)\n        weighted = activated * weights.unsqueeze(-1)\n\n        # Sum across heads: Σ_j w_{t,j} · ReLU(q_{t,j} · k_s)\n        index_scores = weighted.sum(dim=2)\n\n        return index_scores\n\n\nclass AdaptiveTopKSelector(nn.Module):\n    """\n    Adaptive Top-K Token Selection with per-layer k values\n\n    Args:\n        default_top_k: Default k value (can be overridden per forward pass)\n    """\n    def __init__(self, default_top_k: int = 512):\n        super().__init__()\n        self.default_top_k = default_top_k\n\n    def forward(\n        self,\n        index_scores: torch.Tensor,\n        top_k: Optional[int] = None,\n        apply_causal_mask: bool = True\n    ) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:\n        """\n        Select top-k tokens based on index scores\n\n        Args:\n            index_scores: Index scores [batch, seq_len_q, seq_len_k]\n            top_k: Number of tokens to select (overrides default)\n            apply_causal_mask: Whether to apply causal masking\n\n        Returns:\n            - top_k_mask: Boolean mask [batch, seq_len_q, seq_len_k]\n            - top_k_indices: Indices of selected tokens [batch, seq_len_q, k]\n            - stats: Dictionary with selection statistics\n        """\n        batch_size, seq_len_q, seq_len_k = index_scores.shape\n\n        # Use provided k or default\n        k = top_k if top_k is not None else self.default_top_k\n\n        # Apply causal mask: token t can only attend to tokens <= t\n        if apply_causal_mask:\n            causal_mask = torch.triu(\n                torch.ones(seq_len_q, seq_len_k, device=index_scores.device),\n                diagonal=1\n            ).bool()\n            index_scores = index_scores.masked_fill(causal_mask.unsqueeze(0), -1e9)\n\n        # Select top-k indices for each query token\n        actual_k = min(k, seq_len_k)\n        top_k_values, top_k_indices = torch.topk(\n            index_scores,\n            k=actual_k,\n            dim=-1,\n            largest=True\n        )\n\n        # Create boolean mask from indices\n        top_k_mask = torch.zeros_like(index_scores, dtype=torch.bool)\n        top_k_mask.scatter_(2, top_k_indices, True)\n\n        # Compute statistics\n        sparsity = 1.0 - (top_k_mask.sum().item() / top_k_mask.numel())\n        stats = {\n            \'sparsity\': sparsity,\n            \'actual_k\': actual_k,\n            \'k_ratio\': actual_k / seq_len_k\n        }\n\n        return top_k_mask, top_k_indices, stats\n\n\nclass AdaptiveSparseAttention(nn.Module):\n    """\n    DeepSeek Sparse Attention with Adaptive Per-Layer Top-K\n\n    Each layer can have a different sparsity level (k value) based on its\n    functional role in the transformer hierarchy.\n\n    Args:\n        d_model: Model dimension\n        n_heads: Number of attention heads\n        max_seq_len: Maximum sequence length\n        layer_idx: Layer index (0-indexed)\n        layer_top_k: Top-k value for this specific layer\n        indexer_heads: Number of indexer heads\n        indexer_dim: Dimension of indexer queries/keys\n        dropout: Dropout probability\n    """\n    def __init__(\n        self,\n        d_model: int,\n        n_heads: int,\n        max_seq_len: int,\n        layer_idx: int,\n        layer_top_k: int,\n        indexer_heads: int = 4,\n        indexer_dim: int = 64,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.layer_idx = layer_idx\n        self.layer_top_k = layer_top_k\n\n        # Main attention components\n        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)\n        self.w_o = nn.Linear(d_model, d_model, bias=False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.d_k, max_seq_len=max_seq_len, base=10000)\n        self.dropout = dropout\n\n        # Lightning indexer\n        self.indexer = LightningIndexer(\n            d_model=d_model,\n            indexer_heads=indexer_heads,\n            indexer_dim=indexer_dim,\n            dropout=dropout\n        )\n\n        # Adaptive token selector\n        self.selector = AdaptiveTopKSelector(default_top_k=layer_top_k)\n\n        # Whether to use sparse attention\n        self.use_sparse = True\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        return_stats: bool = False\n    ) -> Tuple[torch.Tensor, Optional[Dict]]:\n        """\n        Forward pass with adaptive sparse attention\n\n        Args:\n            x: Input tensor [batch_size, seq_len, d_model]\n            return_stats: Whether to return selection statistics\n\n        Returns:\n            - output: Attention output [batch_size, seq_len, d_model]\n            - stats: Selection statistics if return_stats=True\n        """\n        batch_size, seq_len, _ = x.shape\n\n        # Compute Q, K, V\n        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        Q, K, V = qkv[0], qkv[1], qkv[2]\n\n        # Apply RoPE\n        Q = self.rotary(Q.transpose(1, 2)).transpose(1, 2)\n        K = self.rotary(K.transpose(1, 2)).transpose(1, 2)\n\n        stats = None\n\n        if self.use_sparse:\n            # Compute index scores\n            index_scores = self.indexer(x)\n\n            # Select top-k tokens (using layer-specific k)\n            top_k_mask, top_k_indices, selector_stats = self.selector(\n                index_scores,\n                top_k=self.layer_top_k,\n                apply_causal_mask=True\n            )\n\n            # Create attention mask\n            attn_mask = torch.zeros(\n                batch_size, 1, seq_len, seq_len,\n                device=x.device,\n                dtype=Q.dtype\n            )\n            attn_mask = attn_mask.masked_fill(~top_k_mask.unsqueeze(1), float(\'-inf\'))\n\n            # Apply sparse attention\n            attn_output = F.scaled_dot_product_attention(\n                Q, K, V,\n                attn_mask=attn_mask,\n                dropout_p=self.dropout if self.training else 0.0\n            )\n\n            if return_stats:\n                stats = {\n                    \'layer_idx\': self.layer_idx,\n                    \'layer_k\': self.layer_top_k,\n                    **selector_stats\n                }\n        else:\n            # Dense attention\n            attn_output = F.scaled_dot_product_attention(\n                Q, K, V,\n                is_causal=True,\n                dropout_p=self.dropout if self.training else 0.0\n            )\n\n            if return_stats:\n                stats = {\n                    \'layer_idx\': self.layer_idx,\n                    \'layer_k\': seq_len,\n                    \'sparsity\': 0.0,\n                    \'k_ratio\': 1.0\n                }\n\n        # Reshape and project output\n        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n        output = self.w_o(attn_output)\n\n        return output, stats\n\n    def enable_sparse(self):\n        """Enable sparse attention"""\n        self.use_sparse = True\n\n    def disable_sparse(self):\n        """Disable sparse attention (use dense)"""\n        self.use_sparse = False\n\n    def update_layer_k(self, new_k: int):\n        """Update the layer\'s top-k value dynamically"""\n        self.layer_top_k = new_k\n        self.selector.default_top_k = new_k\n\n\ndef print_schedule_info(config: LayerSparsityConfig, n_layers: int):\n    """Print detailed information about a sparsity schedule"""\n    print(f"\\n{\'=\'*80}")\n    print(f"Sparsity Schedule: {config.schedule_name}")\n    print(f"{\'=\'*80}")\n    print(f"Description: {config.description}")\n    print(f"\\nPer-Layer Configuration:")\n    print(f"{\'Layer\':<10} {\'k Ratio\':<15} {\'Function\':<30}")\n    print(f"{\'-\'*80}")\n\n    for i in range(n_layers):\n        ratio = config.layer_k_ratios[i] if i < len(config.layer_k_ratios) else config.layer_k_ratios[-1]\n\n        # Categorize layer\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n        if i < early_cutoff:\n            function = "Early (local patterns)"\n        elif i < middle_cutoff:\n            function = "Middle (feature composition)"\n        else:\n            function = "Late (global context)"\n\n        print(f"Layer {i:<4} {ratio:<15.2%} {function:<30}")\n    print(f"{\'=\'*80}\\n")\n\n\n# ================= Qwen3-Next Components (Fallback) =================\n\nimport torch.nn.functional as F\nimport math\n\nclass Qwen3NextRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nclass Qwen3NextMLP(nn.Module):\n    def __init__(self, config=None, intermediate_size=512, hidden_size=128):\n        super().__init__()\n        if config:\n            hidden_size = getattr(config, \'hidden_size\', 128)\n            intermediate_size = intermediate_size or hidden_size * 4\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n\n    def forward(self, x):\n        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Qwen3NextSparseMoeBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_dim = config.hidden_size\n        self.num_experts = config.num_experts\n        self.top_k = config.num_experts_per_tok\n        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n        self.experts = nn.ModuleList([\n            Qwen3NextMLP(intermediate_size=config.moe_intermediate_size, hidden_size=self.hidden_dim)\n            for _ in range(self.num_experts)\n        ])\n\n    def forward(self, hidden_states):\n        batch_size, seq_len, hidden_dim = hidden_states.shape\n        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n        router_logits = self.gate(hidden_states_flat)\n        routing_weights = F.softmax(router_logits, dim=1)\n        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n\n        final_hidden_states = torch.zeros_like(hidden_states_flat)\n        for expert_idx in range(self.num_experts):\n            expert_layer = self.experts[expert_idx]\n            expert_mask = (selected_experts == expert_idx).any(dim=-1)\n            if expert_mask.any():\n                expert_input = hidden_states_flat[expert_mask]\n                expert_output = expert_layer(expert_input)\n                token_indices = expert_mask.nonzero(as_tuple=True)[0]\n                expert_positions = (selected_experts[expert_mask] == expert_idx).nonzero(as_tuple=True)[1]\n                weights = routing_weights[expert_mask, expert_positions].unsqueeze(-1)\n                final_hidden_states[expert_mask] += expert_output * weights\n\n        return final_hidden_states.view(batch_size, seq_len, hidden_dim)\n\n\nclass Qwen3NextRotaryEmbedding(nn.Module):\n    """Fallback implementation of Qwen3NextRotaryEmbedding"""\n    def __init__(self, config=None, dim=None, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        if config is not None:\n            self.dim = config.head_dim if hasattr(config, \'head_dim\') else config.hidden_size // config.num_attention_heads\n            self.max_position_embeddings = config.max_position_embeddings\n            self.base = config.rope_theta if hasattr(config, \'rope_theta\') else 10000\n        else:\n            self.dim = dim\n            self.max_position_embeddings = max_position_embeddings\n            self.base = base\n\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n        self.register_buffer("inv_freq", inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [batch_size, seq_len, ...]\n        # position_ids: [batch_size, seq_len]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos()\n        sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass Qwen3NextConfig:\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\n# ==================== EXACT Exp3 PLASAQwen3 Implementation ====================\n\nclass PLASADecoderLayer(nn.Module):\n    """\n    Decoder layer that uses Per-Layer Adaptive Sparse Attention for ALL attention\n    (replaces both full_attention and linear_attention)\n\n    EXACT COPY from exp3_models.py lines 49-110\n    """\n    def __init__(self, config, layer_idx: int, layer_top_k: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.layer_idx = layer_idx\n\n        # Use Per-Layer Adaptive Sparse Attention with layer-specific k\n        self.self_attn = AdaptiveSparseAttention(\n            d_model=config.hidden_size,\n            n_heads=config.num_attention_heads,\n            max_seq_len=config.max_position_embeddings,\n            layer_idx=layer_idx,\n            layer_top_k=layer_top_k,\n            indexer_heads=getattr(config, \'indexer_heads\', 4),\n            indexer_dim=getattr(config, \'indexer_dim\', 64),\n            dropout=config.attention_dropout,\n        )\n\n        # MLP (same as Qwen3-Next)\n        if (layer_idx not in config.mlp_only_layers) and (\n            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n        ):\n            self.mlp = Qwen3NextSparseMoeBlock(config)\n        else:\n            self.mlp = Qwen3NextMLP(config, intermediate_size=config.intermediate_size)\n\n        self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> torch.FloatTensor:\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # DeepSeek Sparse Attention\n        hidden_states, _ = self.self_attn(hidden_states)\n\n        hidden_states = residual + hidden_states\n\n        # MLP\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        if isinstance(hidden_states, tuple):\n            hidden_states, _ = hidden_states\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass PLASAQwen3Model(nn.Module):\n    """\n    Variant 2: All attention layers replaced with Per-Layer Adaptive Sparse Attention\n    Uses PROGRESSIVE_SPARSE schedule: Early=Dense, Middle=L/4, Late=L/2\n\n    EXACT COPY from exp3_models.py lines 202-277\n    """\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n\n        # Create sparsity schedule for per-layer k values\n        sparsity_config = create_sparsity_schedule(\n            schedule=SparsitySchedule.PROGRESSIVE_SPARSE,\n            n_layers=config.num_hidden_layers,\n            seq_len=config.max_position_embeddings\n        )\n\n        # Replace all layers with PLASA decoder layers with layer-specific k values\n        self.layers = nn.ModuleList([\n            PLASADecoderLayer(\n                config,\n                layer_idx,\n                layer_top_k=sparsity_config.get_k_for_layer(layer_idx, config.max_position_embeddings)\n            )\n            for layer_idx in range(config.num_hidden_layers)\n        ])\n\n        self.norm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = Qwen3NextRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ):\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if cache_position is None:\n            past_seen_tokens = 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        hidden_states = inputs_embeds\n        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n\n        for decoder_layer in self.layers:\n            hidden_states = decoder_layer(\n                hidden_states,\n                position_embeddings=position_embeddings,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                cache_position=cache_position,\n                **kwargs,\n            )\n\n        hidden_states = self.norm(hidden_states)\n\n        return type(\'ModelOutput\', (), {\n            \'last_hidden_state\': hidden_states,\n            \'past_key_values\': past_key_values,\n        })()\n\n\nclass PLASAQwen3(nn.Module):\n    """\n    Variant 2: Per-Layer Adaptive Sparse Attention (PLASA)-Only Qwen3 (for CausalLM)\n\n    EXACT COPY from exp3_models.py lines 280-309\n    """\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = PLASAQwen3Model(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n        logits = self.lm_head(outputs.last_hidden_state)\n\n        loss = None\n        if labels is not None:\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n        return type(\'CausalLMOutput\', (), {\n            \'loss\': loss,\n            \'logits\': logits,\n            \'past_key_values\': outputs.past_key_values,\n        })()\n\n\n# Create PLASAModel wrapper for benchmark compatibility\nclass PLASAModel(nn.Module):\n    """\n    Wrapper for PLASAQwen3 that accepts evaluator-style parameters\n    and converts them to a Qwen3NextConfig.\n    """\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_size: int = 128,\n        n_layers: int = 4,\n        n_heads: int = 4,\n        num_kv_heads: int = 2,\n        head_dim: int = 32,\n        intermediate_size: int = 512,\n        max_seq_len: int = 128,\n        dropout: float = 0.1,\n        rms_norm_eps: float = 1e-6,\n    ):\n        super().__init__()\n\n        # Create Qwen3NextConfig from evaluator parameters (matching exp3)\n        config = Qwen3NextConfig(\n            vocab_size=vocab_size,\n            hidden_size=hidden_size,\n            num_hidden_layers=n_layers,\n            num_attention_heads=n_heads,\n            num_key_value_heads=num_kv_heads,\n            head_dim=head_dim,\n            intermediate_size=intermediate_size,\n            max_position_embeddings=512,  # exp3 uses 512, not max_seq_len!\n            rope_theta=10000.0,\n            attention_dropout=dropout,\n            hidden_dropout_prob=dropout,  # exp3 uses this\n            partial_rotary_factor=1.0,  # exp3 uses this\n            rms_norm_eps=rms_norm_eps,\n            pad_token_id=0,\n            # MoE parameters (EXACT match with exp3)\n            num_experts=4,\n            num_local_experts=4,\n            num_experts_per_tok=2,\n            decoder_sparse_step=2,\n            moe_intermediate_size=256,\n            shared_expert_intermediate_size=0,\n            mlp_only_layers=[],\n            # PLASA parameters (EXACT match with exp3)\n            indexer_heads=4,\n            indexer_dim=32,  # exp3 uses 32, not 64!\n        )\n\n        # Set attention implementation (required for full_attention layers)\n        config._attn_implementation = "eager"\n\n        # Create the actual PLASA model\n        self.model = PLASAQwen3(config)\n\n    def forward(self, input_ids, labels=None, **kwargs):\n        return self.model(input_ids=input_ids, labels=labels, **kwargs)\n\n\n__all__ = [\'PLASAModel\', \'AdaptiveSparseAttention\', \'SparsitySchedule\', \'create_sparsity_schedule\']\n\n```\n\n- Current Metrics\nscore=0.0112, perplexity=89.2253, accuracy=0.4952, train_loss=6.3318, val_loss=4.4912\n\nPlease generate a new research proposal that:\n1. Addresses the limitations shown in the current metrics\n2. Incorporates insights from successful approaches\n3. Proposes specific technical improvements\n4. Maintains clarity and technical rigor\n\nReturn the proposal as a clear, concise research abstract.'}], 'model': 'openai/gpt-5.1-codex-mini', 'max_tokens': 8192, 'temperature': 0.7, 'top_p': 0.95}}
2025-11-16 16:33:57,856 - openai._base_client - DEBUG - Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2025-11-16 16:33:57,856 - httpcore.connection - DEBUG - connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None
2025-11-16 16:33:57,914 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734aa6ae40>
2025-11-16 16:33:57,915 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7574c5d4cfd0> server_hostname='openrouter.ai' timeout=5.0
2025-11-16 16:33:57,941 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7574c63b26f0>
2025-11-16 16:33:57,941 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-11-16 16:33:57,941 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-11-16 16:33:57,942 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-11-16 16:33:57,942 - httpcore.http11 - DEBUG - send_request_body.complete
2025-11-16 16:33:57,942 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-11-16 16:33:58,262 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 16 Nov 2025 21:33:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Content-Encoding', b'gzip'), (b'Access-Control-Allow-Origin', b'*'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99fa1c159b5605fa-IAD')])
2025-11-16 16:33:58,262 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-16 16:33:58,263 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-11-16 16:34:04,837 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-11-16 16:34:04,838 - httpcore.http11 - DEBUG - response_closed.started
2025-11-16 16:34:04,838 - httpcore.http11 - DEBUG - response_closed.complete
2025-11-16 16:34:04,838 - openai._base_client - DEBUG - HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "200 OK" Headers({'date': 'Sun, 16 Nov 2025 21:33:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99fa1c159b5605fa-IAD'})
2025-11-16 16:34:04,838 - openai._base_client - DEBUG - request_id: None
2025-11-16 16:34:04,839 - evolve_agent.llm.openai - DEBUG - ====================================================================================================
2025-11-16 16:34:04,839 - evolve_agent.llm.openai - DEBUG - API parameters: You are a research advisor tasked with evolving and improving research proposals. 
Your goal is to generate a new research proposal that builds upon the current proposal while addressing its limitations and incorporating insights from successful approaches.

Focus on:
1. Identifying weaknesses in the current approach based on performance metrics
2. Proposing novel improvements that could enhance performance
3. Learning from successful inspirations while maintaining originality
4. Ensuring the new proposal is technically sound and implementable
Based on the following information, generate an improved research proposal:

- Current Proposal:
Per-Layer Adaptive Sparse Attention (PLASA) Benchmark - November 2025

Objective
=========
Optimize the implementation of Per-Layer Adaptive Sparse Attention (PLASA) to achieve
the lowest validation perplexity on a 4-layer transformer language model trained on
WikiText-2 for 1000 steps.

PLASA uses progressive sparsity scheduling based on layer specialization research:
- Early layers (0-33%): Dense attention (k=L) for local pattern recognition
- Middle layers (33-66%): Aggressive sparse (k=L/4) due to functional redundancy
- Late layers (66-100%): Moderate sparse (k=L/2) for global context consolidation

Background
==========
Recent research (Sep-Nov 2025) has shown that different transformer layers specialize
in distinct functions:

1. Layer Specialization (arXiv:2510.17469, Oct 2025):
   - Early layers: Rapid specialization in pattern recognition and memorization
   - Middle layers: Consolidate in-distribution generalization (but show redundancy)
   - Late layers: Refine for out-of-distribution reasoning and global context

2. Dynamic Attention Mask (DAM) - Oct 2025:
   - Per-layer and per-head dynamic sparse attention masks
   - Context-aware sparsity structures learned from frozen models
   - Achieves long-sequence modeling without retraining

3. DeepSeek Sparse Attention - Lightning Indexer (Nov 2025):
   - Fast, lightweight token selection using FP8 precision
   - Two-stage: approximate indexer → exact attention on top-k
   - Mathematical formulation:
     * Index scores: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)
     * Top-k selection: S_t = TopK_k({ I^(ReLU)_{t,s} })
     * Sparse attention only on selected tokens

Mathematical Formulation
========================
The PLASA implementation must include:

1. Lightning Indexer:
   - Multi-head indexer queries: q_{t,j}^I ∈ R^{d_I} for j=1..H_I
   - Shared indexer keys: k_s^I ∈ R^{d_I}
   - Per-head weights: w_{t,j}
   - Index score: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)

2. Adaptive Top-K Selector:
   - Causal masking: token t can only attend to s ≤ t
   - Per-layer k values from progressive schedule
   - Top-k selection: S_t = TopK_k({ I_{t,s} })

3. Sparse Attention:
   - Standard scaled dot-product attention on selected tokens
   - RoPE (Rotary Position Embeddings) for positional encoding
   - Attention mask from top-k selection

4. Progressive Sparsity Schedule:
   For a 4-layer model with sequence length L=128:
   - Layer 0: k = 128 (100% dense)
   - Layer 1: k = 32  (25% sparse)
   - Layer 2: k = 32  (25% sparse)
   - Layer 3: k = 64  (50% sparse)

Architecture Specifications
============================
Fixed architecture for fair comparison:
- 4 transformer layers (all using PLASA)
- 128 hidden dimensions
- 4 attention heads
- 128 sequence length
- ~1.5M parameters (including indexer)

Training Configuration
======================
- Dataset: WikiText-2 (2M tokens cached)
- Training: 1000 steps
- Batch size: 2
- Learning rate: 3e-4 (AdamW)
- Gradient clipping: 1.0
- Dropout: 0.1

Evaluation Metrics
==================
Primary metric: Validation Perplexity (lower is better)
Scoring: score = 1 / perplexity (higher score is better)

Additional metrics reported:
- Validation loss
- Validation accuracy (next-token prediction)
- Training loss

Baseline Performance (initial_program.py)
==========================================
The provided initial implementation achieves on cosmopedia-v2:
- Validation Perplexity: ~72-80 (expected range)
- Validation Accuracy: ~50-55%
- Score: ~0.0125-0.0139 (1/perplexity)

Dataset: cosmopedia-v2 (HuggingFaceTB/smollm-corpus)
Tokenizer: SmolLM-135M
This matches the exact setup used in exp3_plasa_gdn_hybrid.

This baseline implements the full PLASA algorithm with:
- Lightning Indexer with 4 heads, 32-dim indexer space
- Progressive sparsity schedule (PROGRESSIVE_SPARSE)
- RoPE positional embeddings
- Efficient top-k selection with causal masking

Optimization Goals
==================
Potential improvements to explore:
1. Indexer architecture: Number of heads, dimensionality, activation functions
2. Sparsity schedules: Alternative schedules (AGGRESSIVE_MIDDLE, DENSE_TO_SPARSE)
3. Top-k selection: Dynamic k based on input, learned threshold adaptation
4. Weight initialization: Better initialization for indexer components
5. Regularization: Dropout rates, gradient clipping strategies
6. Efficiency optimizations: Quantization, sparse kernels, fused operations

Constraints
===========
- Must use the progressive sparsity principle (different k per layer)
- Must implement the Lightning Indexer concept (fast token selection)
- Model architecture fixed (4 layers, 128 dim, 4 heads)
- Training budget fixed (1000 steps)
- Must be self-contained in initial_program.py (no external dependencies except PyTorch)

Comparison Context
==================
This benchmark is based on Experiment 3 (exp3_plasa_gdn_hybrid) which showed:
- PLASA with all 4 layers achieved 51.69% accuracy, 73.81 perplexity
- PLASA outperformed full attention by 18.4% (lower loss)
- PLASA outperformed uniform sparse attention (Exp1) by 33.9%
- Training time: 35.5s for 1000 steps (74% faster than hybrids)

The benchmark tests whether alternative implementations can match or exceed this
baseline performance through architectural innovations, better sparsity schedules,
or optimization techniques.

Notes
=====
- Evaluator uses WikiText-2 if available, falls back to synthetic data
- Results cached to avoid reprocessing
- Validation evaluated on 100 batches for speed
- Comparisons should use the same random seed (42) for reproducibility
- Higher score is better (score = 1/perplexity)
- Perplexity capped at 10000 to avoid division issues

References
==========
- DeepSeek Sparse Attention (2025): Lightning Indexer, FP8 quantization
- Dynamic Attention Mask (GitHub: ResponsibleAILab/DAM, Oct 2025)
- Layer Specialization (arXiv:2510.17469, Oct 2025)
- Transformer Layers as Painters (Emergence.ai, Aug 2024-2025)
- Original PLASA implementation (exp3_plasa_gdn_hybrid, 2025)


- Current Program:
```python
"""
Adaptive Per-Layer Sparse Attention Implementation

This module implements sparse attention with layer-specific top-k values.
Based on research showing different layers specialize in different functions:
- Early layers: Local patterns, short-range dependencies
- Middle layers: Feature composition, functionally redundant
- Late layers: Global context consolidation, semantic abstraction

Key Innovation: Each layer has a different sparsity budget (k value) optimized
for its functional role in the transformer hierarchy.

References:
- "Learning to Skip the Middle Layers of Transformers" (2025)
- "Transformer Layers as Painters" - Emergence.ai (2025)
- DeepSeek-V3.2-Exp Lightning Indexer
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchtune.modules import RotaryPositionalEmbeddings
from typing import Optional, Tuple, List, Dict
from dataclasses import dataclass
from enum import Enum


class SparsitySchedule(Enum):
    """Predefined sparsity schedules for different hypotheses"""
    DENSE_BASELINE = "dense_baseline"
    UNIFORM_SPARSE = "uniform_sparse"
    DENSE_TO_SPARSE = "dense_to_sparse"
    AGGRESSIVE_MIDDLE = "aggressive_middle"
    PROGRESSIVE_SPARSE = "progressive_sparse"
    REVERSE_PROGRESSIVE = "reverse_progressive"


@dataclass
class LayerSparsityConfig:
    """Configuration for per-layer sparsity"""
    schedule_name: str
    layer_k_values: List[int]  # k value for each layer
    layer_k_ratios: List[float]  # k as fraction of sequence length
    description: str

    def get_k_for_layer(self, layer_idx: int, seq_len: int) -> int:
        """Get k value for a specific layer"""
        if layer_idx >= len(self.layer_k_ratios):
            # Default to last value if layer index exceeds config
            ratio = self.layer_k_ratios[-1]
        else:
            ratio = self.layer_k_ratios[layer_idx]

        k = int(seq_len * ratio)
        return max(1, min(k, seq_len))  # Clamp to [1, seq_len]


def create_sparsity_schedule(
    schedule: SparsitySchedule,
    n_layers: int,
    seq_len: int
) -> LayerSparsityConfig:
    """
    Create a sparsity schedule based on predefined patterns

    Args:
        schedule: Schedule type
        n_layers: Number of transformer layers
        seq_len: Sequence length

    Returns:
        LayerSparsityConfig with per-layer k values
    """
    if schedule == SparsitySchedule.DENSE_BASELINE:
        # All layers dense (no sparsity)
        ratios = [1.0] * n_layers
        description = "Baseline: All layers dense (k=L)"

    elif schedule == SparsitySchedule.UNIFORM_SPARSE:
        # All layers uniform 50% sparsity (Exp2 baseline)
        ratios = [0.5] * n_layers
        description = "Uniform: All layers k=L/2 (Exp2 baseline)"

    elif schedule == SparsitySchedule.DENSE_TO_SPARSE:
        # Conservative: Dense early, gradually sparse
        # Early (0-33%): Dense (k=L)
        # Middle (33-66%): Moderate sparse (k=L/2)
        # Late (66-100%): Light sparse (k=3L/4)
        ratios = []
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3

        for i in range(n_layers):
            if i < early_cutoff:
                ratios.append(1.0)  # Dense
            elif i < middle_cutoff:
                ratios.append(0.5)  # Moderate sparse
            else:
                ratios.append(0.75)  # Light sparse
        description = "Dense-to-Sparse: Early=Dense, Middle=L/2, Late=3L/4"

    elif schedule == SparsitySchedule.AGGRESSIVE_MIDDLE:
        # Based on redundancy research: Middle layers most sparse
        # Early: Moderate (k=L/2)
        # Middle: Aggressive (k=L/4) - most redundant
        # Late: Moderate (k=L/2)
        ratios = []
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3

        for i in range(n_layers):
            if i < early_cutoff:
                ratios.append(0.5)  # Moderate
            elif i < middle_cutoff:
                ratios.append(0.25)  # Aggressive sparse
            else:
                ratios.append(0.5)  # Moderate
        description = "Aggressive-Middle: Early=L/2, Middle=L/4, Late=L/2"

    elif schedule == SparsitySchedule.PROGRESSIVE_SPARSE:
        # Original hypothesis: Dense foundation, aggressive middle, moderate late
        # Early: Dense (k=L)
        # Middle: Aggressive (k=L/4)
        # Late: Moderate (k=L/2)
        ratios = []
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3

        for i in range(n_layers):
            if i < early_cutoff:
                ratios.append(1.0)  # Dense
            elif i < middle_cutoff:
                ratios.append(0.25)  # Aggressive sparse
            else:
                ratios.append(0.5)  # Moderate
        description = "Progressive-Sparse: Early=Dense, Middle=L/4, Late=L/2"

    else:
        raise ValueError(f"Unknown schedule: {schedule}")

    # Compute actual k values
    k_values = [int(seq_len * ratio) for ratio in ratios]

    return LayerSparsityConfig(
        schedule_name=schedule.value,
        layer_k_values=k_values,
        layer_k_ratios=ratios,
        description=description
    )


class LightningIndexer(nn.Module):
    """
    Lightning Indexer for DeepSeek Sparse Attention

    Computes index scores I_{t,s} = Σ w_{t,j} · ReLU(q_{t,j} · k_s)

    Args:
        d_model: Model dimension
        indexer_heads: Number of indexer heads (H_I)
        indexer_dim: Dimension of indexer queries/keys (d_I)
        dropout: Dropout probability
    """
    def __init__(
        self,
        d_model: int,
        indexer_heads: int = 4,
        indexer_dim: int = 64,
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_model = d_model
        self.indexer_heads = indexer_heads
        self.indexer_dim = indexer_dim

        # Indexer query projection: h_t -> {q_{t,j}^I}
        self.q_proj = nn.Linear(d_model, indexer_heads * indexer_dim, bias=False)

        # Indexer key projection: h_s -> k_s^I
        self.k_proj = nn.Linear(d_model, indexer_dim, bias=False)

        # Indexer weights: w_{t,j}^I for each head
        self.w_proj = nn.Linear(d_model, indexer_heads, bias=False)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compute index scores between all pairs of tokens

        Args:
            x: Input tensor [batch_size, seq_len, d_model]

        Returns:
            index_scores: Index scores [batch_size, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape

        # Compute indexer queries: [batch, seq_len, indexer_heads, indexer_dim]
        queries = self.q_proj(x).reshape(batch_size, seq_len, self.indexer_heads, self.indexer_dim)

        # Compute indexer keys: [batch, seq_len, indexer_dim]
        keys = self.k_proj(x)

        # Compute indexer weights: [batch, seq_len, indexer_heads]
        weights = self.w_proj(x)

        # Compute dot products: q_{t,j} · k_s for all t, s, j
        dots = torch.einsum('bthd,bsd->bths', queries, keys)

        # Apply ReLU activation
        activated = F.relu(dots)

        # Weight each head: w_{t,j} · ReLU(q_{t,j} · k_s)
        weighted = activated * weights.unsqueeze(-1)

        # Sum across heads: Σ_j w_{t,j} · ReLU(q_{t,j} · k_s)
        index_scores = weighted.sum(dim=2)

        return index_scores


class AdaptiveTopKSelector(nn.Module):
    """
    Adaptive Top-K Token Selection with per-layer k values

    Args:
        default_top_k: Default k value (can be overridden per forward pass)
    """
    def __init__(self, default_top_k: int = 512):
        super().__init__()
        self.default_top_k = default_top_k

    def forward(
        self,
        index_scores: torch.Tensor,
        top_k: Optional[int] = None,
        apply_causal_mask: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:
        """
        Select top-k tokens based on index scores

        Args:
            index_scores: Index scores [batch, seq_len_q, seq_len_k]
            top_k: Number of tokens to select (overrides default)
            apply_causal_mask: Whether to apply causal masking

        Returns:
            - top_k_mask: Boolean mask [batch, seq_len_q, seq_len_k]
            - top_k_indices: Indices of selected tokens [batch, seq_len_q, k]
            - stats: Dictionary with selection statistics
        """
        batch_size, seq_len_q, seq_len_k = index_scores.shape

        # Use provided k or default
        k = top_k if top_k is not None else self.default_top_k

        # Apply causal mask: token t can only attend to tokens <= t
        if apply_causal_mask:
            causal_mask = torch.triu(
                torch.ones(seq_len_q, seq_len_k, device=index_scores.device),
                diagonal=1
            ).bool()
            index_scores = index_scores.masked_fill(causal_mask.unsqueeze(0), -1e9)

        # Select top-k indices for each query token
        actual_k = min(k, seq_len_k)
        top_k_values, top_k_indices = torch.topk(
            index_scores,
            k=actual_k,
            dim=-1,
            largest=True
        )

        # Create boolean mask from indices
        top_k_mask = torch.zeros_like(index_scores, dtype=torch.bool)
        top_k_mask.scatter_(2, top_k_indices, True)

        # Compute statistics
        sparsity = 1.0 - (top_k_mask.sum().item() / top_k_mask.numel())
        stats = {
            'sparsity': sparsity,
            'actual_k': actual_k,
            'k_ratio': actual_k / seq_len_k
        }

        return top_k_mask, top_k_indices, stats


class AdaptiveSparseAttention(nn.Module):
    """
    DeepSeek Sparse Attention with Adaptive Per-Layer Top-K

    Each layer can have a different sparsity level (k value) based on its
    functional role in the transformer hierarchy.

    Args:
        d_model: Model dimension
        n_heads: Number of attention heads
        max_seq_len: Maximum sequence length
        layer_idx: Layer index (0-indexed)
        layer_top_k: Top-k value for this specific layer
        indexer_heads: Number of indexer heads
        indexer_dim: Dimension of indexer queries/keys
        dropout: Dropout probability
    """
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        max_seq_len: int,
        layer_idx: int,
        layer_top_k: int,
        indexer_heads: int = 4,
        indexer_dim: int = 64,
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.layer_idx = layer_idx
        self.layer_top_k = layer_top_k

        # Main attention components
        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        self.rotary = RotaryPositionalEmbeddings(dim=self.d_k, max_seq_len=max_seq_len, base=10000)
        self.dropout = dropout

        # Lightning indexer
        self.indexer = LightningIndexer(
            d_model=d_model,
            indexer_heads=indexer_heads,
            indexer_dim=indexer_dim,
            dropout=dropout
        )

        # Adaptive token selector
        self.selector = AdaptiveTopKSelector(default_top_k=layer_top_k)

        # Whether to use sparse attention
        self.use_sparse = True

    def forward(
        self,
        x: torch.Tensor,
        return_stats: bool = False
    ) -> Tuple[torch.Tensor, Optional[Dict]]:
        """
        Forward pass with adaptive sparse attention

        Args:
            x: Input tensor [batch_size, seq_len, d_model]
            return_stats: Whether to return selection statistics

        Returns:
            - output: Attention output [batch_size, seq_len, d_model]
            - stats: Selection statistics if return_stats=True
        """
        batch_size, seq_len, _ = x.shape

        # Compute Q, K, V
        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        Q, K, V = qkv[0], qkv[1], qkv[2]

        # Apply RoPE
        Q = self.rotary(Q.transpose(1, 2)).transpose(1, 2)
        K = self.rotary(K.transpose(1, 2)).transpose(1, 2)

        stats = None

        if self.use_sparse:
            # Compute index scores
            index_scores = self.indexer(x)

            # Select top-k tokens (using layer-specific k)
            top_k_mask, top_k_indices, selector_stats = self.selector(
                index_scores,
                top_k=self.layer_top_k,
                apply_causal_mask=True
            )

            # Create attention mask
            attn_mask = torch.zeros(
                batch_size, 1, seq_len, seq_len,
                device=x.device,
                dtype=Q.dtype
            )
            attn_mask = attn_mask.masked_fill(~top_k_mask.unsqueeze(1), float('-inf'))

            # Apply sparse attention
            attn_output = F.scaled_dot_product_attention(
                Q, K, V,
                attn_mask=attn_mask,
                dropout_p=self.dropout if self.training else 0.0
            )

            if return_stats:
                stats = {
                    'layer_idx': self.layer_idx,
                    'layer_k': self.layer_top_k,
                    **selector_stats
                }
        else:
            # Dense attention
            attn_output = F.scaled_dot_product_attention(
                Q, K, V,
                is_causal=True,
                dropout_p=self.dropout if self.training else 0.0
            )

            if return_stats:
                stats = {
                    'layer_idx': self.layer_idx,
                    'layer_k': seq_len,
                    'sparsity': 0.0,
                    'k_ratio': 1.0
                }

        # Reshape and project output
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)
        output = self.w_o(attn_output)

        return output, stats

    def enable_sparse(self):
        """Enable sparse attention"""
        self.use_sparse = True

    def disable_sparse(self):
        """Disable sparse attention (use dense)"""
        self.use_sparse = False

    def update_layer_k(self, new_k: int):
        """Update the layer's top-k value dynamically"""
        self.layer_top_k = new_k
        self.selector.default_top_k = new_k


def print_schedule_info(config: LayerSparsityConfig, n_layers: int):
    """Print detailed information about a sparsity schedule"""
    print(f"\n{'='*80}")
    print(f"Sparsity Schedule: {config.schedule_name}")
    print(f"{'='*80}")
    print(f"Description: {config.description}")
    print(f"\nPer-Layer Configuration:")
    print(f"{'Layer':<10} {'k Ratio':<15} {'Function':<30}")
    print(f"{'-'*80}")

    for i in range(n_layers):
        ratio = config.layer_k_ratios[i] if i < len(config.layer_k_ratios) else config.layer_k_ratios[-1]

        # Categorize layer
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3
        if i < early_cutoff:
            function = "Early (local patterns)"
        elif i < middle_cutoff:
            function = "Middle (feature composition)"
        else:
            function = "Late (global context)"

        print(f"Layer {i:<4} {ratio:<15.2%} {function:<30}")
    print(f"{'='*80}\n")


# ================= Qwen3-Next Components (Fallback) =================

import torch.nn.functional as F
import math

class Qwen3NextRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)


class Qwen3NextMLP(nn.Module):
    def __init__(self, config=None, intermediate_size=512, hidden_size=128):
        super().__init__()
        if config:
            hidden_size = getattr(config, 'hidden_size', 128)
            intermediate_size = intermediate_size or hidden_size * 4
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))


class Qwen3NextSparseMoeBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config.hidden_size
        self.num_experts = config.num_experts
        self.top_k = config.num_experts_per_tok
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)
        self.experts = nn.ModuleList([
            Qwen3NextMLP(intermediate_size=config.moe_intermediate_size, hidden_size=self.hidden_dim)
            for _ in range(self.num_experts)
        ])

    def forward(self, hidden_states):
        batch_size, seq_len, hidden_dim = hidden_states.shape
        hidden_states_flat = hidden_states.view(-1, hidden_dim)
        router_logits = self.gate(hidden_states_flat)
        routing_weights = F.softmax(router_logits, dim=1)
        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)

        final_hidden_states = torch.zeros_like(hidden_states_flat)
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            expert_mask = (selected_experts == expert_idx).any(dim=-1)
            if expert_mask.any():
                expert_input = hidden_states_flat[expert_mask]
                expert_output = expert_layer(expert_input)
                token_indices = expert_mask.nonzero(as_tuple=True)[0]
                expert_positions = (selected_experts[expert_mask] == expert_idx).nonzero(as_tuple=True)[1]
                weights = routing_weights[expert_mask, expert_positions].unsqueeze(-1)
                final_hidden_states[expert_mask] += expert_output * weights

        return final_hidden_states.view(batch_size, seq_len, hidden_dim)


class Qwen3NextRotaryEmbedding(nn.Module):
    """Fallback implementation of Qwen3NextRotaryEmbedding"""
    def __init__(self, config=None, dim=None, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        if config is not None:
            self.dim = config.head_dim if hasattr(config, 'head_dim') else config.hidden_size // config.num_attention_heads
            self.max_position_embeddings = config.max_position_embeddings
            self.base = config.rope_theta if hasattr(config, 'rope_theta') else 10000
        else:
            self.dim = dim
            self.max_position_embeddings = max_position_embeddings
            self.base = base

        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    @torch.no_grad()
    def forward(self, x, position_ids):
        # x: [batch_size, seq_len, ...]
        # position_ids: [batch_size, seq_len]
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        position_ids_expanded = position_ids[:, None, :].float()

        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos()
        sin = emb.sin()
        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


class Qwen3NextConfig:
    def __init__(self, **kwargs):
        for k, v in kwargs.items():
            setattr(self, k, v)


# ==================== EXACT Exp3 PLASAQwen3 Implementation ====================

class PLASADecoderLayer(nn.Module):
    """
    Decoder layer that uses Per-Layer Adaptive Sparse Attention for ALL attention
    (replaces both full_attention and linear_attention)

    EXACT COPY from exp3_models.py lines 49-110
    """
    def __init__(self, config, layer_idx: int, layer_top_k: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx

        # Use Per-Layer Adaptive Sparse Attention with layer-specific k
        self.self_attn = AdaptiveSparseAttention(
            d_model=config.hidden_size,
            n_heads=config.num_attention_heads,
            max_seq_len=config.max_position_embeddings,
            layer_idx=layer_idx,
            layer_top_k=layer_top_k,
            indexer_heads=getattr(config, 'indexer_heads', 4),
            indexer_dim=getattr(config, 'indexer_dim', 64),
            dropout=config.attention_dropout,
        )

        # MLP (same as Qwen3-Next)
        if (layer_idx not in config.mlp_only_layers) and (
            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0
        ):
            self.mlp = Qwen3NextSparseMoeBlock(config)
        else:
            self.mlp = Qwen3NextMLP(config, intermediate_size=config.intermediate_size)

        self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> torch.FloatTensor:
        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # DeepSeek Sparse Attention
        hidden_states, _ = self.self_attn(hidden_states)

        hidden_states = residual + hidden_states

        # MLP
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        if isinstance(hidden_states, tuple):
            hidden_states, _ = hidden_states
        hidden_states = residual + hidden_states

        return hidden_states


class PLASAQwen3Model(nn.Module):
    """
    Variant 2: All attention layers replaced with Per-Layer Adaptive Sparse Attention
    Uses PROGRESSIVE_SPARSE schedule: Early=Dense, Middle=L/4, Late=L/2

    EXACT COPY from exp3_models.py lines 202-277
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)

        # Create sparsity schedule for per-layer k values
        sparsity_config = create_sparsity_schedule(
            schedule=SparsitySchedule.PROGRESSIVE_SPARSE,
            n_layers=config.num_hidden_layers,
            seq_len=config.max_position_embeddings
        )

        # Replace all layers with PLASA decoder layers with layer-specific k values
        self.layers = nn.ModuleList([
            PLASADecoderLayer(
                config,
                layer_idx,
                layer_top_k=sparsity_config.get_k_for_layer(layer_idx, config.max_position_embeddings)
            )
            for layer_idx in range(config.num_hidden_layers)
        ])

        self.norm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3NextRotaryEmbedding(config=config)
        self.gradient_checkpointing = False

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ):
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )
        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        for decoder_layer in self.layers:
            hidden_states = decoder_layer(
                hidden_states,
                position_embeddings=position_embeddings,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                cache_position=cache_position,
                **kwargs,
            )

        hidden_states = self.norm(hidden_states)

        return type('ModelOutput', (), {
            'last_hidden_state': hidden_states,
            'past_key_values': past_key_values,
        })()


class PLASAQwen3(nn.Module):
    """
    Variant 2: Per-Layer Adaptive Sparse Attention (PLASA)-Only Qwen3 (for CausalLM)

    EXACT COPY from exp3_models.py lines 280-309
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.model = PLASAQwen3Model(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.LongTensor] = None,
        **kwargs,
    ):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        logits = self.lm_head(outputs.last_hidden_state)

        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        return type('CausalLMOutput', (), {
            'loss': loss,
            'logits': logits,
            'past_key_values': outputs.past_key_values,
        })()


# Create PLASAModel wrapper for benchmark compatibility
class PLASAModel(nn.Module):
    """
    Wrapper for PLASAQwen3 that accepts evaluator-style parameters
    and converts them to a Qwen3NextConfig.
    """
    def __init__(
        self,
        vocab_size: int,
        hidden_size: int = 128,
        n_layers: int = 4,
        n_heads: int = 4,
        num_kv_heads: int = 2,
        head_dim: int = 32,
        intermediate_size: int = 512,
        max_seq_len: int = 128,
        dropout: float = 0.1,
        rms_norm_eps: float = 1e-6,
    ):
        super().__init__()

        # Create Qwen3NextConfig from evaluator parameters (matching exp3)
        config = Qwen3NextConfig(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_hidden_layers=n_layers,
            num_attention_heads=n_heads,
            num_key_value_heads=num_kv_heads,
            head_dim=head_dim,
            intermediate_size=intermediate_size,
            max_position_embeddings=512,  # exp3 uses 512, not max_seq_len!
            rope_theta=10000.0,
            attention_dropout=dropout,
            hidden_dropout_prob=dropout,  # exp3 uses this
            partial_rotary_factor=1.0,  # exp3 uses this
            rms_norm_eps=rms_norm_eps,
            pad_token_id=0,
            # MoE parameters (EXACT match with exp3)
            num_experts=4,
            num_local_experts=4,
            num_experts_per_tok=2,
            decoder_sparse_step=2,
            moe_intermediate_size=256,
            shared_expert_intermediate_size=0,
            mlp_only_layers=[],
            # PLASA parameters (EXACT match with exp3)
            indexer_heads=4,
            indexer_dim=32,  # exp3 uses 32, not 64!
        )

        # Set attention implementation (required for full_attention layers)
        config._attn_implementation = "eager"

        # Create the actual PLASA model
        self.model = PLASAQwen3(config)

    def forward(self, input_ids, labels=None, **kwargs):
        return self.model(input_ids=input_ids, labels=labels, **kwargs)


__all__ = ['PLASAModel', 'AdaptiveSparseAttention', 'SparsitySchedule', 'create_sparsity_schedule']

```

- Current Metrics
score=0.0112, perplexity=89.2253, accuracy=0.4952, train_loss=6.3318, val_loss=4.4912

Please generate a new research proposal that:
1. Addresses the limitations shown in the current metrics
2. Incorporates insights from successful approaches
3. Proposes specific technical improvements
4. Maintains clarity and technical rigor

Return the proposal as a clear, concise research abstract.
2025-11-16 16:34:04,839 - evolve_agent.llm.openai - DEBUG - ====================================================================================================
2025-11-16 16:34:04,839 - evolve_agent.llm.openai - DEBUG - API response: **Title:**  
Dynamic Per-Layer Sparse Calibration (DPSC) for PLASA: Adaptive k-Budgeting and Context-Aware Indexing for Lower Perplexity

**Abstract:**  
The current PLASA implementation achieves only marginal gains over dense baselines (validation perplexity ≈89, accuracy ≈49%, 1/ppx ≈0.0112), indicating residual inefficiencies in static sparsity budgets, coarse indexer dynamics, and limited adaptation to token-specific context. We propose Dynamic Per-Layer Sparse Calibration (DPSC), a refinement that retains the progressive sparsity principle and Lightning Indexer while introducing finer-grained, learnable mechanisms inspired by Dynamic Attention Masking (DAM) and DeepSeek’s two-stage selection. DPSC (1) augments each layer with a lightweight gating network that predicts adaptive \(k_t^{(l)}\) per token based on contextual entropy and attention footprint, allowing middle layers to further reduce redundancy without harming downstream coherence; (2) replaces fixed top-k thresholds with learned per-layer percentile estimators trained via a small auxiliary objective that encourages calibration to true attention mass, mimicking DAM’s context-aware masks; (3) transforms the Lightning Indexer into a dual-path indexer where an FP8 approximate scoring stage filters candidates before a refined FP16 re-ranking, reducing false positives and making sparse masks tighter; (4) regularizes indexer learning through initialization tied to scaled RoPE statistics and stochastic depth on selection heads to improve gradient flow. Training-level improvements include a cosine learning-rate warmup, adaptive dropout scaling per layer, and self-distillation from a dense-attention teacher on a cached WikiText-2 subset to stabilize early training. We will evaluate DPSC on the same 4-layer, 128-dim PLASA model training regime (1000 steps, WikiText-2) and measure improvements in validation perplexity, accuracy, and sparsity efficiency over the baseline. By blending dynamic scheduling, context-aware selection, and resilient regularization, DPSC aims to lower perplexity below 70 while respecting the benchmark’s architectural and computational constraints.
2025-11-16 16:34:04,840 - evolve_agent.llm.openai - DEBUG - ====================================================================================================
2025-11-16 16:34:04,840 - evolve_agent.controller - INFO - Generated new proposal for evolution round 0
2025-11-16 16:34:04,840 - evolve_agent.reward_model - INFO - Scoring 1 research proposals...
2025-11-16 16:34:04,840 - evolve_agent.reward_model - DEBUG - Scoring attempt 1/6 for: proposal
2025-11-16 16:34:04,841 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-64a6952b-774b-4a15-9b0a-0f1c4daf7da8', 'json_data': {'messages': [{'role': 'system', 'content': 'You are an expert reviewer tasked with evaluating the quality of a research proposal.\nYour evaluations must be consistent, objective, and based on clear criteria.'}, {'role': 'user', 'content': 'Carefully evaluate the following research proposal and assign a score from 1 to 10.\n\nEvaluation Criteria:\n- Clarity: Is the proposal well-written and easy to understand?\n- Novelty: Does it introduce new ideas or approaches?\n- Technical Rigor: Is the methodology sound and well-justified?\n- Potential Impact: Could this research make a significant contribution?\n\nScoring Guidelines:\n- Scores 1-3: Poor quality, major flaws\n- Scores 4-5: Below average, significant issues\n- Score 6: Slightly above borderline, acceptable\n- Scores 7-8: Good quality, solid contribution\n- Scores 9-10: Excellent, exceptional contribution\n\nYou MUST respond with valid JSON in this exact format:\n{\n  "score": <integer 1-10>,\n  "explanation": "<your detailed evaluation reasoning>"\n}\n\nResearch Proposal:\n**Title:**  \nDynamic Per-Layer Sparse Calibration (DPSC) for PLASA: Adaptive k-Budgeting and Context-Aware Indexing for Lower Perplexity\n\n**Abstract:**  \nThe current PLASA implementation achieves only marginal gains over dense baselines (validation perplexity ≈89, accuracy ≈49%, 1/ppx ≈0.0112), indicating residual inefficiencies in static sparsity budgets, coarse indexer dynamics, and limited adaptation to token-specific context. We propose Dynamic Per-Layer Sparse Calibration (DPSC), a refinement that retains the progressive sparsity principle and Lightning Indexer while introducing finer-grained, learnable mechanisms inspired by Dynamic Attention Masking (DAM) and DeepSeek’s two-stage selection. DPSC (1) augments each layer with a lightweight gating network that predicts adaptive \\(k_t^{(l)}\\) per token based on contextual entropy and attention footprint, allowing middle layers to further reduce redundancy without harming downstream coherence; (2) replaces fixed top-k thresholds with learned per-layer percentile estimators trained via a small auxiliary objective that encourages calibration to true attention mass, mimicking DAM’s context-aware masks; (3) transforms the Lightning Indexer into a dual-path indexer where an FP8 approximate scoring stage filters candidates before a refined FP16 re-ranking, reducing false positives and making sparse masks tighter; (4) regularizes indexer learning through initialization tied to scaled RoPE statistics and stochastic depth on selection heads to improve gradient flow. Training-level improvements include a cosine learning-rate warmup, adaptive dropout scaling per layer, and self-distillation from a dense-attention teacher on a cached WikiText-2 subset to stabilize early training. We will evaluate DPSC on the same 4-layer, 128-dim PLASA model training regime (1000 steps, WikiText-2) and measure improvements in validation perplexity, accuracy, and sparsity efficiency over the baseline. By blending dynamic scheduling, context-aware selection, and resilient regularization, DPSC aims to lower perplexity below 70 while respecting the benchmark’s architectural and computational constraints.\n'}], 'model': 'moonshotai/kimi-k2-thinking', 'max_tokens': 4096, 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'research_proposal_score', 'schema': {'type': 'object', 'properties': {'score': {'type': 'integer', 'description': 'Numerical score from 1 to 10', 'minimum': 1, 'maximum': 10}, 'explanation': {'type': 'string', 'description': 'Detailed reasoning for the score'}}, 'required': ['score', 'explanation'], 'additionalProperties': False}, 'strict': True}}, 'temperature': 0.3, 'top_p': 0.95}}
2025-11-16 16:34:04,841 - openai._base_client - DEBUG - Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2025-11-16 16:34:04,842 - httpcore.connection - DEBUG - close.started
2025-11-16 16:34:04,842 - httpcore.connection - DEBUG - close.complete
2025-11-16 16:34:04,842 - httpcore.connection - DEBUG - connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=60.0 socket_options=None
2025-11-16 16:34:04,874 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734aa6bad0>
2025-11-16 16:34:04,874 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7574c5d4d3d0> server_hostname='openrouter.ai' timeout=60.0
2025-11-16 16:34:04,896 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734aa69e50>
2025-11-16 16:34:04,896 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-11-16 16:34:04,897 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-11-16 16:34:04,897 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-11-16 16:34:04,897 - httpcore.http11 - DEBUG - send_request_body.complete
2025-11-16 16:34:04,897 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-11-16 16:34:05,951 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 16 Nov 2025 21:34:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Content-Encoding', b'gzip'), (b'Access-Control-Allow-Origin', b'*'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99fa1c42d9676cf9-IAD')])
2025-11-16 16:34:05,951 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-16 16:34:05,952 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-11-16 16:34:33,913 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-11-16 16:34:33,914 - httpcore.http11 - DEBUG - response_closed.started
2025-11-16 16:34:33,914 - httpcore.http11 - DEBUG - response_closed.complete
2025-11-16 16:34:33,914 - openai._base_client - DEBUG - HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "200 OK" Headers({'date': 'Sun, 16 Nov 2025 21:34:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99fa1c42d9676cf9-IAD'})
2025-11-16 16:34:33,914 - openai._base_client - DEBUG - request_id: None
2025-11-16 16:34:33,915 - evolve_agent.reward_model - DEBUG - Successfully parsed JSON score: 6.0
2025-11-16 16:34:33,915 - evolve_agent.reward_model - INFO - Successfully scored '': 6.0/10
2025-11-16 16:34:33,917 - evolve_agent.reward_model - INFO - Completed scoring: 1/1 successful
2025-11-16 16:34:33,918 - evolve_agent.controller - INFO - Iteration 1: Proposal score: 6.0000 / 10
2025-11-16 16:34:33,919 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-41b900ed-7f2c-4c82-b728-81b305be4334', 'json_data': {'messages': [{'role': 'system', 'content': "You are an expert software developer tasked with iteratively improving a codebase.\nYour job is to analyze the current program and suggest improvements based on feedback from previous attempts.\nFocus on making targeted changes that will increase the program's performance metrics.\n"}, {'role': 'user', 'content': '# Previous Proposal: \nPer-Layer Adaptive Sparse Attention (PLASA) Benchmark - November 2025\n\nObjective\n=========\nOptimize the implementation of Per-Layer Adaptive Sparse Attention (PLASA) to achieve\nthe lowest validation perplexity on a 4-layer transformer language model trained on\nWikiText-2 for 1000 steps.\n\nPLASA uses progressive sparsity scheduling based on layer specialization research:\n- Early layers (0-33%): Dense attention (k=L) for local pattern recognition\n- Middle layers (33-66%): Aggressive sparse (k=L/4) due to functional redundancy\n- Late layers (66-100%): Moderate sparse (k=L/2) for global context consolidation\n\nBackground\n==========\nRecent research (Sep-Nov 2025) has shown that different transformer layers specialize\nin distinct functions:\n\n1. Layer Specialization (arXiv:2510.17469, Oct 2025):\n   - Early layers: Rapid specialization in pattern recognition and memorization\n   - Middle layers: Consolidate in-distribution generalization (but show redundancy)\n   - Late layers: Refine for out-of-distribution reasoning and global context\n\n2. Dynamic Attention Mask (DAM) - Oct 2025:\n   - Per-layer and per-head dynamic sparse attention masks\n   - Context-aware sparsity structures learned from frozen models\n   - Achieves long-sequence modeling without retraining\n\n3. DeepSeek Sparse Attention - Lightning Indexer (Nov 2025):\n   - Fast, lightweight token selection using FP8 precision\n   - Two-stage: approximate indexer → exact attention on top-k\n   - Mathematical formulation:\n     * Index scores: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)\n     * Top-k selection: S_t = TopK_k({ I^(ReLU)_{t,s} })\n     * Sparse attention only on selected tokens\n\nMathematical Formulation\n========================\nThe PLASA implementation must include:\n\n1. Lightning Indexer:\n   - Multi-head indexer queries: q_{t,j}^I ∈ R^{d_I} for j=1..H_I\n   - Shared indexer keys: k_s^I ∈ R^{d_I}\n   - Per-head weights: w_{t,j}\n   - Index score: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)\n\n2. Adaptive Top-K Selecto...(truncated)\n\n# Previous Program:\n```python\n"""\nAdaptive Per-Layer Sparse Attention Implementation\n\nThis module implements sparse attention with layer-specific top-k values.\nBased on research showing different layers specialize in different functions:\n- Early layers: Local patterns, short-range dependencies\n- Middle layers: Feature composition, functionally redundant\n- Late layers: Global context consolidation, semantic abstraction\n\nKey Innovation: Each layer has a different sparsity budget (k value) optimized\nfor its functional role in the transformer hierarchy.\n\nReferences:\n- "Learning to Skip the Middle Layers of Transformers" (2025)\n- "Transformer Layers as Painters" - Emergence.ai (2025)\n- DeepSeek-V3.2-Exp Lightning Indexer\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtune.modules import RotaryPositionalEmbeddings\nfrom typing import Optional, Tuple, List, Dict\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass SparsitySchedule(Enum):\n    """Predefined sparsity schedules for different hypotheses"""\n    DENSE_BASELINE = "dense_baseline"\n    UNIFORM_SPARSE = "uniform_sparse"\n    DENSE_TO_SPARSE = "dense_to_sparse"\n    AGGRESSIVE_MIDDLE = "aggressive_middle"\n    PROGRESSIVE_SPARSE = "progressive_sparse"\n    REVERSE_PROGRESSIVE = "reverse_progressive"\n\n\n@dataclass\nclass LayerSparsityConfig:\n    """Configuration for per-layer sparsity"""\n    schedule_name: str\n    layer_k_values: List[int]  # k value for each layer\n    layer_k_ratios: List[float]  # k as fraction of sequence length\n    description: str\n\n    def get_k_for_layer(self, layer_idx: int, seq_len: int) -> int:\n        """Get k value for a specific layer"""\n        if layer_idx >= len(self.layer_k_ratios):\n            # Default to last value if layer index exceeds config\n            ratio = self.layer_k_ratios[-1]\n        else:\n            ratio = self.layer_k_ratios[layer_idx]\n\n        k = int(seq_len * ratio)\n        return max(1, min(k, seq_len))  # Clamp to [1, seq_len]\n\n\ndef create_sparsity_schedule(\n    schedule: SparsitySchedule,\n    n_layers: int,\n    seq_len: int\n) -> LayerSparsityConfig:\n    """\n    Create a sparsity schedule based on predefined patterns\n\n    Args:\n        schedule: Schedule type\n        n_layers: Number of transformer layers\n        seq_len: Sequence length\n\n    Returns:\n        LayerSparsityConfig with per-layer k values\n    """\n    if schedule == SparsitySchedule.DENSE_BASELINE:\n        # All layers dense (no sparsity)\n        ratios = [1.0] * n_layers\n        description = "Baseline: All layers dense (k=L)"\n\n    elif schedule == SparsitySchedule.UNIFORM_SPARSE:\n        # All layers uniform 50% sparsity (Exp2 baseline)\n        ratios = [0.5] * n_layers\n        description = "Uniform: All layers k=L/2 (Exp2 baseline)"\n\n    elif schedule == SparsitySchedule.DENSE_TO_SPARSE:\n        # Conservative: Dense early, gradually sparse\n        # Early (0-33%): Dense (k=L)\n        # Middle (33-66%): Moderate sparse (k=L/2)\n        # Late (66-100%): Light sparse (k=3L/4)\n        ratios = []\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n\n        for i in range(n_layers):\n            if i < early_cutoff:\n                ratios.append(1.0)  # Dense\n            elif i < middle_cutoff:\n                ratios.append(0.5)  # Moderate sparse\n            else:\n                ratios.append(0.75)  # Light sparse\n        description = "Dense-to-Sparse: Early=Dense, Middle=L/2, Late=3L/4"\n\n    elif schedule == SparsitySchedule.AGGRESSIVE_MIDDLE:\n        # Based on redundancy research: Middle layers most sparse\n        # Early: Moderate (k=L/2)\n        # Middle: Aggressive (k=L/4) - most redundant\n        # Late: Moderate (k=L/2)\n        ratios = []\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n\n        for i in range(n_layers):\n            if i < early_cutoff:\n                ratios.append(0.5)  # Moderate\n            elif i < middle_cutoff:\n                ratios.append(0.25)  # Aggressive sparse\n            else:\n                ratios.append(0.5)  # Moderate\n        description = "Aggressive-Middle: Early=L/2, Middle=L/4, Late=L/2"\n\n    elif schedule == SparsitySchedule.PROGRESSIVE_SPARSE:\n        # Original hypothesis: Dense foundation, aggressive middle, moderate late\n        # Early: Dense (k=L)\n        # Middle: Aggressive (k=L/4)\n        # Late: Moderate (k=L/2)\n        ratios = []\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n\n        for i in range(n_layers):\n            if i < early_cutoff:\n                ratios.append(1.0)  # Dense\n            elif i < middle_cutoff:\n                ratios.append(0.25)  # Aggressive sparse\n            else:\n                ratios.append(0.5)  # Moderate\n        description = "Progressive-Sparse: Early=Dense, Middle=L/4, Late=L/2"\n\n    else:\n        raise ValueError(f"Unknown schedule: {schedule}")\n\n    # Compute actual k values\n    k_values = [int(seq_len * ratio) for ratio in ratios]\n\n    return LayerSparsityConfig(\n        schedule_name=schedule.value,\n        layer_k_values=k_values,\n        layer_k_ratios=ratios,\n        description=description\n    )\n\n\nclass LightningIndexer(nn.Module):\n    """\n    Lightning Indexer for DeepSeek Sparse Attention\n\n    Computes index scores I_{t,s} = Σ w_{t,j} · ReLU(q_{t,j} · k_s)\n\n    Args:\n        d_model: Model dimension\n        indexer_heads: Number of indexer heads (H_I)\n        indexer_dim: Dimension of indexer queries/keys (d_I)\n        dropout: Dropout probability\n    """\n    def __init__(\n        self,\n        d_model: int,\n        indexer_heads: int = 4,\n        indexer_dim: int = 64,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.indexer_heads = indexer_heads\n        self.indexer_dim = indexer_dim\n\n        # Indexer query projection: h_t -> {q_{t,j}^I}\n        self.q_proj = nn.Linear(d_model, indexer_heads * indexer_dim, bias=False)\n\n        # Indexer key projection: h_s -> k_s^I\n        self.k_proj = nn.Linear(d_model, indexer_dim, bias=False)\n\n        # Indexer weights: w_{t,j}^I for each head\n        self.w_proj = nn.Linear(d_model, indexer_heads, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """\n        Compute index scores between all pairs of tokens\n\n        Args:\n            x: Input tensor [batch_size, seq_len, d_model]\n\n        Returns:\n            index_scores: Index scores [batch_size, seq_len, seq_len]\n        """\n        batch_size, seq_len, _ = x.shape\n\n        # Compute indexer queries: [batch, seq_len, indexer_heads, indexer_dim]\n        queries = self.q_proj(x).reshape(batch_size, seq_len, self.indexer_heads, self.indexer_dim)\n\n        # Compute indexer keys: [batch, seq_len, indexer_dim]\n        keys = self.k_proj(x)\n\n        # Compute indexer weights: [batch, seq_len, indexer_heads]\n        weights = self.w_proj(x)\n\n        # Compute dot products: q_{t,j} · k_s for all t, s, j\n        dots = torch.einsum(\'bthd,bsd->bths\', queries, keys)\n\n        # Apply ReLU activation\n        activated = F.relu(dots)\n\n        # Weight each head: w_{t,j} · ReLU(q_{t,j} · k_s)\n        weighted = activated * weights.unsqueeze(-1)\n\n        # Sum across heads: Σ_j w_{t,j} · ReLU(q_{t,j} · k_s)\n        index_scores = weighted.sum(dim=2)\n\n        return index_scores\n\n\nclass AdaptiveTopKSelector(nn.Module):\n    """\n    Adaptive Top-K Token Selection with per-layer k values\n\n    Args:\n        default_top_k: Default k value (can be overridden per forward pass)\n    """\n    def __init__(self, default_top_k: int = 512):\n        super().__init__()\n        self.default_top_k = default_top_k\n\n    def forward(\n        self,\n        index_scores: torch.Tensor,\n        top_k: Optional[int] = None,\n        apply_causal_mask: bool = True\n    ) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:\n        """\n        Select top-k tokens based on index scores\n\n        Args:\n            index_scores: Index scores [batch, seq_len_q, seq_len_k]\n            top_k: Number of tokens to select (overrides default)\n            apply_causal_mask: Whether to apply causal masking\n\n        Returns:\n            - top_k_mask: Boolean mask [batch, seq_len_q, seq_len_k]\n            - top_k_indices: Indices of selected tokens [batch, seq_len_q, k]\n            - stats: Dictionary with selection statistics\n        """\n        batch_size, seq_len_q, seq_len_k = index_scores.shape\n\n        # Use provided k or default\n        k = top_k if top_k is not None else self.default_top_k\n\n        # Apply causal mask: token t can only attend to tokens <= t\n        if apply_causal_mask:\n            causal_mask = torch.triu(\n                torch.ones(seq_len_q, seq_len_k, device=index_scores.device),\n                diagonal=1\n            ).bool()\n            index_scores = index_scores.masked_fill(causal_mask.unsqueeze(0), -1e9)\n\n        # Select top-k indices for each query token\n        actual_k = min(k, seq_len_k)\n        top_k_values, top_k_indices = torch.topk(\n            index_scores,\n            k=actual_k,\n            dim=-1,\n            largest=True\n        )\n\n        # Create boolean mask from indices\n        top_k_mask = torch.zeros_like(index_scores, dtype=torch.bool)\n        top_k_mask.scatter_(2, top_k_indices, True)\n\n        # Compute statistics\n        sparsity = 1.0 - (top_k_mask.sum().item() / top_k_mask.numel())\n        stats = {\n            \'sparsity\': sparsity,\n            \'actual_k\': actual_k,\n            \'k_ratio\': actual_k / seq_len_k\n        }\n\n        return top_k_mask, top_k_indices, stats\n\n\nclass AdaptiveSparseAttention(nn.Module):\n    """\n    DeepSeek Sparse Attention with Adaptive Per-Layer Top-K\n\n    Each layer can have a different sparsity level (k value) based on its\n    functional role in the transformer hierarchy.\n\n    Args:\n        d_model: Model dimension\n        n_heads: Number of attention heads\n        max_seq_len: Maximum sequence length\n        layer_idx: Layer index (0-indexed)\n        layer_top_k: Top-k value for this specific layer\n        indexer_heads: Number of indexer heads\n        indexer_dim: Dimension of indexer queries/keys\n        dropout: Dropout probability\n    """\n    def __init__(\n        self,\n        d_model: int,\n        n_heads: int,\n        max_seq_len: int,\n        layer_idx: int,\n        layer_top_k: int,\n        indexer_heads: int = 4,\n        indexer_dim: int = 64,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.layer_idx = layer_idx\n        self.layer_top_k = layer_top_k\n\n        # Main attention components\n        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)\n        self.w_o = nn.Linear(d_model, d_model, bias=False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.d_k, max_seq_len=max_seq_len, base=10000)\n        self.dropout = dropout\n\n        # Lightning indexer\n        self.indexer = LightningIndexer(\n            d_model=d_model,\n            indexer_heads=indexer_heads,\n            indexer_dim=indexer_dim,\n            dropout=dropout\n        )\n\n        # Adaptive token selector\n        self.selector = AdaptiveTopKSelector(default_top_k=layer_top_k)\n\n        # Whether to use sparse attention\n        self.use_sparse = True\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        return_stats: bool = False\n    ) -> Tuple[torch.Tensor, Optional[Dict]]:\n        """\n        Forward pass with adaptive sparse attention\n\n        Args:\n            x: Input tensor [batch_size, seq_len, d_model]\n            return_stats: Whether to return selection statistics\n\n        Returns:\n            - output: Attention output [batch_size, seq_len, d_model]\n            - stats: Selection statistics if return_stats=True\n        """\n        batch_size, seq_len, _ = x.shape\n\n        # Compute Q, K, V\n        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        Q, K, V = qkv[0], qkv[1], qkv[2]\n\n        # Apply RoPE\n        Q = self.rotary(Q.transpose(1, 2)).transpose(1, 2)\n        K = self.rotary(K.transpose(1, 2)).transpose(1, 2)\n\n        stats = None\n\n        if self.use_sparse:\n            # Compute index scores\n            index_scores = self.indexer(x)\n\n            # Select top-k tokens (using layer-specific k)\n            top_k_mask, top_k_indices, selector_stats = self.selector(\n                index_scores,\n                top_k=self.layer_top_k,\n                apply_causal_mask=True\n            )\n\n            # Create attention mask\n            attn_mask = torch.zeros(\n                batch_size, 1, seq_len, seq_len,\n                device=x.device,\n                dtype=Q.dtype\n            )\n            attn_mask = attn_mask.masked_fill(~top_k_mask.unsqueeze(1), float(\'-inf\'))\n\n            # Apply sparse attention\n            attn_output = F.scaled_dot_product_attention(\n                Q, K, V,\n                attn_mask=attn_mask,\n                dropout_p=self.dropout if self.training else 0.0\n            )\n\n            if return_stats:\n                stats = {\n                    \'layer_idx\': self.layer_idx,\n                    \'layer_k\': self.layer_top_k,\n                    **selector_stats\n                }\n        else:\n            # Dense attention\n            attn_output = F.scaled_dot_product_attention(\n                Q, K, V,\n                is_causal=True,\n                dropout_p=self.dropout if self.training else 0.0\n            )\n\n            if return_stats:\n                stats = {\n                    \'layer_idx\': self.layer_idx,\n                    \'layer_k\': seq_len,\n                    \'sparsity\': 0.0,\n                    \'k_ratio\': 1.0\n                }\n\n        # Reshape and project output\n        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n        output = self.w_o(attn_output)\n\n        return output, stats\n\n    def enable_sparse(self):\n        """Enable sparse attention"""\n        self.use_sparse = True\n\n    def disable_sparse(self):\n        """Disable sparse attention (use dense)"""\n        self.use_sparse = False\n\n    def update_layer_k(self, new_k: int):\n        """Update the layer\'s top-k value dynamically"""\n        self.layer_top_k = new_k\n        self.selector.default_top_k = new_k\n\n\ndef print_schedule_info(config: LayerSparsityConfig, n_layers: int):\n    """Print detailed information about a sparsity schedule"""\n    print(f"\\n{\'=\'*80}")\n    print(f"Sparsity Schedule: {config.schedule_name}")\n    print(f"{\'=\'*80}")\n    print(f"Description: {config.description}")\n    print(f"\\nPer-Layer Configuration:")\n    print(f"{\'Layer\':<10} {\'k Ratio\':<15} {\'Function\':<30}")\n    print(f"{\'-\'*80}")\n\n    for i in range(n_layers):\n        ratio = config.layer_k_ratios[i] if i < len(config.layer_k_ratios) else config.layer_k_ratios[-1]\n\n        # Categorize layer\n        early_cutoff = n_layers // 3\n        middle_cutoff = 2 * n_layers // 3\n        if i < early_cutoff:\n            function = "Early (local patterns)"\n        elif i < middle_cutoff:\n            function = "Middle (feature composition)"\n        else:\n            function = "Late (global context)"\n\n        print(f"Layer {i:<4} {ratio:<15.2%} {function:<30}")\n    print(f"{\'=\'*80}\\n")\n\n\n# ================= Qwen3-Next Components (Fallback) =================\n\nimport torch.nn.functional as F\nimport math\n\nclass Qwen3NextRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nclass Qwen3NextMLP(nn.Module):\n    def __init__(self, config=None, intermediate_size=512, hidden_size=128):\n        super().__init__()\n        if config:\n            hidden_size = getattr(config, \'hidden_size\', 128)\n            intermediate_size = intermediate_size or hidden_size * 4\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n\n    def forward(self, x):\n        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Qwen3NextSparseMoeBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_dim = config.hidden_size\n        self.num_experts = config.num_experts\n        self.top_k = config.num_experts_per_tok\n        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n        self.experts = nn.ModuleList([\n            Qwen3NextMLP(intermediate_size=config.moe_intermediate_size, hidden_size=self.hidden_dim)\n            for _ in range(self.num_experts)\n        ])\n\n    def forward(self, hidden_states):\n        batch_size, seq_len, hidden_dim = hidden_states.shape\n        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n        router_logits = self.gate(hidden_states_flat)\n        routing_weights = F.softmax(router_logits, dim=1)\n        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n\n        final_hidden_states = torch.zeros_like(hidden_states_flat)\n        for expert_idx in range(self.num_experts):\n            expert_layer = self.experts[expert_idx]\n            expert_mask = (selected_experts == expert_idx).any(dim=-1)\n            if expert_mask.any():\n                expert_input = hidden_states_flat[expert_mask]\n                expert_output = expert_layer(expert_input)\n                token_indices = expert_mask.nonzero(as_tuple=True)[0]\n                expert_positions = (selected_experts[expert_mask] == expert_idx).nonzero(as_tuple=True)[1]\n                weights = routing_weights[expert_mask, expert_positions].unsqueeze(-1)\n                final_hidden_states[expert_mask] += expert_output * weights\n\n        return final_hidden_states.view(batch_size, seq_len, hidden_dim)\n\n\nclass Qwen3NextRotaryEmbedding(nn.Module):\n    """Fallback implementation of Qwen3NextRotaryEmbedding"""\n    def __init__(self, config=None, dim=None, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        if config is not None:\n            self.dim = config.head_dim if hasattr(config, \'head_dim\') else config.hidden_size // config.num_attention_heads\n            self.max_position_embeddings = config.max_position_embeddings\n            self.base = config.rope_theta if hasattr(config, \'rope_theta\') else 10000\n        else:\n            self.dim = dim\n            self.max_position_embeddings = max_position_embeddings\n            self.base = base\n\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n        self.register_buffer("inv_freq", inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [batch_size, seq_len, ...]\n        # position_ids: [batch_size, seq_len]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos()\n        sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass Qwen3NextConfig:\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\n# ==================== EXACT Exp3 PLASAQwen3 Implementation ====================\n\nclass PLASADecoderLayer(nn.Module):\n    """\n    Decoder layer that uses Per-Layer Adaptive Sparse Attention for ALL attention\n    (replaces both full_attention and linear_attention)\n\n    EXACT COPY from exp3_models.py lines 49-110\n    """\n    def __init__(self, config, layer_idx: int, layer_top_k: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.layer_idx = layer_idx\n\n        # Use Per-Layer Adaptive Sparse Attention with layer-specific k\n        self.self_attn = AdaptiveSparseAttention(\n            d_model=config.hidden_size,\n            n_heads=config.num_attention_heads,\n            max_seq_len=config.max_position_embeddings,\n            layer_idx=layer_idx,\n            layer_top_k=layer_top_k,\n            indexer_heads=getattr(config, \'indexer_heads\', 4),\n            indexer_dim=getattr(config, \'indexer_dim\', 64),\n            dropout=config.attention_dropout,\n        )\n\n        # MLP (same as Qwen3-Next)\n        if (layer_idx not in config.mlp_only_layers) and (\n            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n        ):\n            self.mlp = Qwen3NextSparseMoeBlock(config)\n        else:\n            self.mlp = Qwen3NextMLP(config, intermediate_size=config.intermediate_size)\n\n        self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> torch.FloatTensor:\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # DeepSeek Sparse Attention\n        hidden_states, _ = self.self_attn(hidden_states)\n\n        hidden_states = residual + hidden_states\n\n        # MLP\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        if isinstance(hidden_states, tuple):\n            hidden_states, _ = hidden_states\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass PLASAQwen3Model(nn.Module):\n    """\n    Variant 2: All attention layers replaced with Per-Layer Adaptive Sparse Attention\n    Uses PROGRESSIVE_SPARSE schedule: Early=Dense, Middle=L/4, Late=L/2\n\n    EXACT COPY from exp3_models.py lines 202-277\n    """\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n\n        # Create sparsity schedule for per-layer k values\n        sparsity_config = create_sparsity_schedule(\n            schedule=SparsitySchedule.PROGRESSIVE_SPARSE,\n            n_layers=config.num_hidden_layers,\n            seq_len=config.max_position_embeddings\n        )\n\n        # Replace all layers with PLASA decoder layers with layer-specific k values\n        self.layers = nn.ModuleList([\n            PLASADecoderLayer(\n                config,\n                layer_idx,\n                layer_top_k=sparsity_config.get_k_for_layer(layer_idx, config.max_position_embeddings)\n            )\n            for layer_idx in range(config.num_hidden_layers)\n        ])\n\n        self.norm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = Qwen3NextRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ):\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if cache_position is None:\n            past_seen_tokens = 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        hidden_states = inputs_embeds\n        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n\n        for decoder_layer in self.layers:\n            hidden_states = decoder_layer(\n                hidden_states,\n                position_embeddings=position_embeddings,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                cache_position=cache_position,\n                **kwargs,\n            )\n\n        hidden_states = self.norm(hidden_states)\n\n        return type(\'ModelOutput\', (), {\n            \'last_hidden_state\': hidden_states,\n            \'past_key_values\': past_key_values,\n        })()\n\n\nclass PLASAQwen3(nn.Module):\n    """\n    Variant 2: Per-Layer Adaptive Sparse Attention (PLASA)-Only Qwen3 (for CausalLM)\n\n    EXACT COPY from exp3_models.py lines 280-309\n    """\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = PLASAQwen3Model(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n        logits = self.lm_head(outputs.last_hidden_state)\n\n        loss = None\n        if labels is not None:\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n        return type(\'CausalLMOutput\', (), {\n            \'loss\': loss,\n            \'logits\': logits,\n            \'past_key_values\': outputs.past_key_values,\n        })()\n\n\n# Create PLASAModel wrapper for benchmark compatibility\nclass PLASAModel(nn.Module):\n    """\n    Wrapper for PLASAQwen3 that accepts evaluator-style parameters\n    and converts them to a Qwen3NextConfig.\n    """\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_size: int = 128,\n        n_layers: int = 4,\n        n_heads: int = 4,\n        num_kv_heads: int = 2,\n        head_dim: int = 32,\n        intermediate_size: int = 512,\n        max_seq_len: int = 128,\n        dropout: float = 0.1,\n        rms_norm_eps: float = 1e-6,\n    ):\n        super().__init__()\n\n        # Create Qwen3NextConfig from evaluator parameters (matching exp3)\n        config = Qwen3NextConfig(\n            vocab_size=vocab_size,\n            hidden_size=hidden_size,\n            num_hidden_layers=n_layers,\n            num_attention_heads=n_heads,\n            num_key_value_heads=num_kv_heads,\n            head_dim=head_dim,\n            intermediate_size=intermediate_size,\n            max_position_embeddings=512,  # exp3 uses 512, not max_seq_len!\n            rope_theta=10000.0,\n            attention_dropout=dropout,\n            hidden_dropout_prob=dropout,  # exp3 uses this\n            partial_rotary_factor=1.0,  # exp3 uses this\n            rms_norm_eps=rms_norm_eps,\n            pad_token_id=0,\n            # MoE parameters (EXACT match with exp3)\n            num_experts=4,\n            num_local_experts=4,\n            num_experts_per_tok=2,\n            decoder_sparse_step=2,\n            moe_intermediate_size=256,\n            shared_expert_intermediate_size=0,\n            mlp_only_layers=[],\n            # PLASA parameters (EXACT match with exp3)\n            indexer_heads=4,\n            indexer_dim=32,  # exp3 uses 32, not 64!\n        )\n\n        # Set attention implementation (required for full_attention layers)\n        config._attn_implementation = "eager"\n\n        # Create the actual PLASA model\n        self.model = PLASAQwen3(config)\n\n    def forward(self, input_ids, labels=None, **kwargs):\n        return self.model(input_ids=input_ids, labels=labels, **kwargs)\n\n\n__all__ = [\'PLASAModel\', \'AdaptiveSparseAttention\', \'SparsitySchedule\', \'create_sparsity_schedule\']\n\n```\n\n# Previous Performance Metrics: \n- score: 0.0112\n- perplexity: 89.2253\n- accuracy: 0.4952\n- train_loss: 6.3318\n- val_loss: 4.4912\n\n# Areas Identified for Improvement: \n- Consider simplifying the code to improve readability and maintainability\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: score: 0.0112, perplexity: 89.2253, accuracy: 0.4952, train_loss: 6.3318, val_loss: 4.4912\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n\n\n\n# Current Proposal\n**Title:**  \nDynamic Per-Layer Sparse Calibration (DPSC) for PLASA: Adaptive k-Budgeting and Context-Aware Indexing for Lower Perplexity\n\n**Abstract:**  \nThe current PLASA implementation achieves only marginal gains over dense baselines (validation perplexity ≈89, accuracy ≈49%, 1/ppx ≈0.0112), indicating residual inefficiencies in static sparsity budgets, coarse indexer dynamics, and limited adaptation to token-specific context. We propose Dynamic Per-Layer Sparse Calibration (DPSC), a refinement that retains the progressive sparsity principle and Lightning Indexer while introducing finer-grained, learnable mechanisms inspired by Dynamic Attention Masking (DAM) and DeepSeek’s two-stage selection. DPSC (1) augments each layer with a lightweight gating network that predicts adaptive \\(k_t^{(l)}\\) per token based on contextual entropy and attention footprint, allowing middle layers to further reduce redundancy without harming downstream coherence; (2) replaces fixed top-k thresholds with learned per-layer percentile estimators trained via a small auxiliary objective that encourages calibration to true attention mass, mimicking DAM’s context-aware masks; (3) transforms the Lightning Indexer into a dual-path indexer where an FP8 approximate scoring stage filters candidates before a refined FP16 re-ranking, reducing false positives and making sparse masks tighter; (4) regularizes indexer learning through initialization tied to scaled RoPE statistics and stochastic depth on selection heads to improve gradient flow. Training-level improvements include a cosine learning-rate warmup, adaptive dropout scaling per layer, and self-distillation from a dense-attention teacher on a cached WikiText-2 subset to stabilize early training. We will evaluate DPSC on the same 4-layer, 128-dim PLASA model training regime (1000 steps, WikiText-2) and measure improvements in validation perplexity, accuracy, and sparsity efficiency over the baseline. By blending dynamic scheduling, context-aw...(truncated)\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n'}], 'model': 'openai/gpt-5.1-codex-mini', 'max_tokens': 8192, 'temperature': 0.7, 'top_p': 0.95}}
2025-11-16 16:34:33,920 - openai._base_client - DEBUG - Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
2025-11-16 16:34:33,920 - httpcore.connection - DEBUG - close.started
2025-11-16 16:34:33,920 - httpcore.connection - DEBUG - close.complete
2025-11-16 16:34:33,921 - httpcore.connection - DEBUG - connect_tcp.started host='openrouter.ai' port=443 local_address=None timeout=5.0 socket_options=None
2025-11-16 16:34:33,952 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734aaa1520>
2025-11-16 16:34:33,952 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7574c5d4cfd0> server_hostname='openrouter.ai' timeout=5.0
2025-11-16 16:34:33,976 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75734aaa11f0>
2025-11-16 16:34:33,976 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-11-16 16:34:33,976 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-11-16 16:34:33,977 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-11-16 16:34:33,977 - httpcore.http11 - DEBUG - send_request_body.complete
2025-11-16 16:34:33,977 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-11-16 16:34:34,320 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 16 Nov 2025 21:34:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Content-Encoding', b'gzip'), (b'Access-Control-Allow-Origin', b'*'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99fa1cf81c3c432f-IAD')])
2025-11-16 16:34:34,320 - httpx - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-16 16:34:34,321 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-11-16 16:35:15,244 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-11-16 16:35:15,244 - httpcore.http11 - DEBUG - response_closed.started
2025-11-16 16:35:15,244 - httpcore.http11 - DEBUG - response_closed.complete
2025-11-16 16:35:15,245 - openai._base_client - DEBUG - HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "200 OK" Headers({'date': 'Sun, 16 Nov 2025 21:34:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99fa1cf81c3c432f-IAD'})
2025-11-16 16:35:15,245 - openai._base_client - DEBUG - request_id: None
2025-11-16 16:35:15,245 - evolve_agent.llm.openai - DEBUG - ====================================================================================================
2025-11-16 16:35:15,246 - evolve_agent.llm.openai - DEBUG - API parameters: You are an expert software developer tasked with iteratively improving a codebase.
Your job is to analyze the current program and suggest improvements based on feedback from previous attempts.
Focus on making targeted changes that will increase the program's performance metrics.

# Previous Proposal: 
Per-Layer Adaptive Sparse Attention (PLASA) Benchmark - November 2025

Objective
=========
Optimize the implementation of Per-Layer Adaptive Sparse Attention (PLASA) to achieve
the lowest validation perplexity on a 4-layer transformer language model trained on
WikiText-2 for 1000 steps.

PLASA uses progressive sparsity scheduling based on layer specialization research:
- Early layers (0-33%): Dense attention (k=L) for local pattern recognition
- Middle layers (33-66%): Aggressive sparse (k=L/4) due to functional redundancy
- Late layers (66-100%): Moderate sparse (k=L/2) for global context consolidation

Background
==========
Recent research (Sep-Nov 2025) has shown that different transformer layers specialize
in distinct functions:

1. Layer Specialization (arXiv:2510.17469, Oct 2025):
   - Early layers: Rapid specialization in pattern recognition and memorization
   - Middle layers: Consolidate in-distribution generalization (but show redundancy)
   - Late layers: Refine for out-of-distribution reasoning and global context

2. Dynamic Attention Mask (DAM) - Oct 2025:
   - Per-layer and per-head dynamic sparse attention masks
   - Context-aware sparsity structures learned from frozen models
   - Achieves long-sequence modeling without retraining

3. DeepSeek Sparse Attention - Lightning Indexer (Nov 2025):
   - Fast, lightweight token selection using FP8 precision
   - Two-stage: approximate indexer → exact attention on top-k
   - Mathematical formulation:
     * Index scores: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)
     * Top-k selection: S_t = TopK_k({ I^(ReLU)_{t,s} })
     * Sparse attention only on selected tokens

Mathematical Formulation
========================
The PLASA implementation must include:

1. Lightning Indexer:
   - Multi-head indexer queries: q_{t,j}^I ∈ R^{d_I} for j=1..H_I
   - Shared indexer keys: k_s^I ∈ R^{d_I}
   - Per-head weights: w_{t,j}
   - Index score: I_{t,s} = Σ_j w_{t,j} · ReLU(q_{t,j}^I · k_s^I)

2. Adaptive Top-K Selecto...(truncated)

# Previous Program:
```python
"""
Adaptive Per-Layer Sparse Attention Implementation

This module implements sparse attention with layer-specific top-k values.
Based on research showing different layers specialize in different functions:
- Early layers: Local patterns, short-range dependencies
- Middle layers: Feature composition, functionally redundant
- Late layers: Global context consolidation, semantic abstraction

Key Innovation: Each layer has a different sparsity budget (k value) optimized
for its functional role in the transformer hierarchy.

References:
- "Learning to Skip the Middle Layers of Transformers" (2025)
- "Transformer Layers as Painters" - Emergence.ai (2025)
- DeepSeek-V3.2-Exp Lightning Indexer
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchtune.modules import RotaryPositionalEmbeddings
from typing import Optional, Tuple, List, Dict
from dataclasses import dataclass
from enum import Enum


class SparsitySchedule(Enum):
    """Predefined sparsity schedules for different hypotheses"""
    DENSE_BASELINE = "dense_baseline"
    UNIFORM_SPARSE = "uniform_sparse"
    DENSE_TO_SPARSE = "dense_to_sparse"
    AGGRESSIVE_MIDDLE = "aggressive_middle"
    PROGRESSIVE_SPARSE = "progressive_sparse"
    REVERSE_PROGRESSIVE = "reverse_progressive"


@dataclass
class LayerSparsityConfig:
    """Configuration for per-layer sparsity"""
    schedule_name: str
    layer_k_values: List[int]  # k value for each layer
    layer_k_ratios: List[float]  # k as fraction of sequence length
    description: str

    def get_k_for_layer(self, layer_idx: int, seq_len: int) -> int:
        """Get k value for a specific layer"""
        if layer_idx >= len(self.layer_k_ratios):
            # Default to last value if layer index exceeds config
            ratio = self.layer_k_ratios[-1]
        else:
            ratio = self.layer_k_ratios[layer_idx]

        k = int(seq_len * ratio)
        return max(1, min(k, seq_len))  # Clamp to [1, seq_len]


def create_sparsity_schedule(
    schedule: SparsitySchedule,
    n_layers: int,
    seq_len: int
) -> LayerSparsityConfig:
    """
    Create a sparsity schedule based on predefined patterns

    Args:
        schedule: Schedule type
        n_layers: Number of transformer layers
        seq_len: Sequence length

    Returns:
        LayerSparsityConfig with per-layer k values
    """
    if schedule == SparsitySchedule.DENSE_BASELINE:
        # All layers dense (no sparsity)
        ratios = [1.0] * n_layers
        description = "Baseline: All layers dense (k=L)"

    elif schedule == SparsitySchedule.UNIFORM_SPARSE:
        # All layers uniform 50% sparsity (Exp2 baseline)
        ratios = [0.5] * n_layers
        description = "Uniform: All layers k=L/2 (Exp2 baseline)"

    elif schedule == SparsitySchedule.DENSE_TO_SPARSE:
        # Conservative: Dense early, gradually sparse
        # Early (0-33%): Dense (k=L)
        # Middle (33-66%): Moderate sparse (k=L/2)
        # Late (66-100%): Light sparse (k=3L/4)
        ratios = []
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3

        for i in range(n_layers):
            if i < early_cutoff:
                ratios.append(1.0)  # Dense
            elif i < middle_cutoff:
                ratios.append(0.5)  # Moderate sparse
            else:
                ratios.append(0.75)  # Light sparse
        description = "Dense-to-Sparse: Early=Dense, Middle=L/2, Late=3L/4"

    elif schedule == SparsitySchedule.AGGRESSIVE_MIDDLE:
        # Based on redundancy research: Middle layers most sparse
        # Early: Moderate (k=L/2)
        # Middle: Aggressive (k=L/4) - most redundant
        # Late: Moderate (k=L/2)
        ratios = []
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3

        for i in range(n_layers):
            if i < early_cutoff:
                ratios.append(0.5)  # Moderate
            elif i < middle_cutoff:
                ratios.append(0.25)  # Aggressive sparse
            else:
                ratios.append(0.5)  # Moderate
        description = "Aggressive-Middle: Early=L/2, Middle=L/4, Late=L/2"

    elif schedule == SparsitySchedule.PROGRESSIVE_SPARSE:
        # Original hypothesis: Dense foundation, aggressive middle, moderate late
        # Early: Dense (k=L)
        # Middle: Aggressive (k=L/4)
        # Late: Moderate (k=L/2)
        ratios = []
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3

        for i in range(n_layers):
            if i < early_cutoff:
                ratios.append(1.0)  # Dense
            elif i < middle_cutoff:
                ratios.append(0.25)  # Aggressive sparse
            else:
                ratios.append(0.5)  # Moderate
        description = "Progressive-Sparse: Early=Dense, Middle=L/4, Late=L/2"

    else:
        raise ValueError(f"Unknown schedule: {schedule}")

    # Compute actual k values
    k_values = [int(seq_len * ratio) for ratio in ratios]

    return LayerSparsityConfig(
        schedule_name=schedule.value,
        layer_k_values=k_values,
        layer_k_ratios=ratios,
        description=description
    )


class LightningIndexer(nn.Module):
    """
    Lightning Indexer for DeepSeek Sparse Attention

    Computes index scores I_{t,s} = Σ w_{t,j} · ReLU(q_{t,j} · k_s)

    Args:
        d_model: Model dimension
        indexer_heads: Number of indexer heads (H_I)
        indexer_dim: Dimension of indexer queries/keys (d_I)
        dropout: Dropout probability
    """
    def __init__(
        self,
        d_model: int,
        indexer_heads: int = 4,
        indexer_dim: int = 64,
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_model = d_model
        self.indexer_heads = indexer_heads
        self.indexer_dim = indexer_dim

        # Indexer query projection: h_t -> {q_{t,j}^I}
        self.q_proj = nn.Linear(d_model, indexer_heads * indexer_dim, bias=False)

        # Indexer key projection: h_s -> k_s^I
        self.k_proj = nn.Linear(d_model, indexer_dim, bias=False)

        # Indexer weights: w_{t,j}^I for each head
        self.w_proj = nn.Linear(d_model, indexer_heads, bias=False)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compute index scores between all pairs of tokens

        Args:
            x: Input tensor [batch_size, seq_len, d_model]

        Returns:
            index_scores: Index scores [batch_size, seq_len, seq_len]
        """
        batch_size, seq_len, _ = x.shape

        # Compute indexer queries: [batch, seq_len, indexer_heads, indexer_dim]
        queries = self.q_proj(x).reshape(batch_size, seq_len, self.indexer_heads, self.indexer_dim)

        # Compute indexer keys: [batch, seq_len, indexer_dim]
        keys = self.k_proj(x)

        # Compute indexer weights: [batch, seq_len, indexer_heads]
        weights = self.w_proj(x)

        # Compute dot products: q_{t,j} · k_s for all t, s, j
        dots = torch.einsum('bthd,bsd->bths', queries, keys)

        # Apply ReLU activation
        activated = F.relu(dots)

        # Weight each head: w_{t,j} · ReLU(q_{t,j} · k_s)
        weighted = activated * weights.unsqueeze(-1)

        # Sum across heads: Σ_j w_{t,j} · ReLU(q_{t,j} · k_s)
        index_scores = weighted.sum(dim=2)

        return index_scores


class AdaptiveTopKSelector(nn.Module):
    """
    Adaptive Top-K Token Selection with per-layer k values

    Args:
        default_top_k: Default k value (can be overridden per forward pass)
    """
    def __init__(self, default_top_k: int = 512):
        super().__init__()
        self.default_top_k = default_top_k

    def forward(
        self,
        index_scores: torch.Tensor,
        top_k: Optional[int] = None,
        apply_causal_mask: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:
        """
        Select top-k tokens based on index scores

        Args:
            index_scores: Index scores [batch, seq_len_q, seq_len_k]
            top_k: Number of tokens to select (overrides default)
            apply_causal_mask: Whether to apply causal masking

        Returns:
            - top_k_mask: Boolean mask [batch, seq_len_q, seq_len_k]
            - top_k_indices: Indices of selected tokens [batch, seq_len_q, k]
            - stats: Dictionary with selection statistics
        """
        batch_size, seq_len_q, seq_len_k = index_scores.shape

        # Use provided k or default
        k = top_k if top_k is not None else self.default_top_k

        # Apply causal mask: token t can only attend to tokens <= t
        if apply_causal_mask:
            causal_mask = torch.triu(
                torch.ones(seq_len_q, seq_len_k, device=index_scores.device),
                diagonal=1
            ).bool()
            index_scores = index_scores.masked_fill(causal_mask.unsqueeze(0), -1e9)

        # Select top-k indices for each query token
        actual_k = min(k, seq_len_k)
        top_k_values, top_k_indices = torch.topk(
            index_scores,
            k=actual_k,
            dim=-1,
            largest=True
        )

        # Create boolean mask from indices
        top_k_mask = torch.zeros_like(index_scores, dtype=torch.bool)
        top_k_mask.scatter_(2, top_k_indices, True)

        # Compute statistics
        sparsity = 1.0 - (top_k_mask.sum().item() / top_k_mask.numel())
        stats = {
            'sparsity': sparsity,
            'actual_k': actual_k,
            'k_ratio': actual_k / seq_len_k
        }

        return top_k_mask, top_k_indices, stats


class AdaptiveSparseAttention(nn.Module):
    """
    DeepSeek Sparse Attention with Adaptive Per-Layer Top-K

    Each layer can have a different sparsity level (k value) based on its
    functional role in the transformer hierarchy.

    Args:
        d_model: Model dimension
        n_heads: Number of attention heads
        max_seq_len: Maximum sequence length
        layer_idx: Layer index (0-indexed)
        layer_top_k: Top-k value for this specific layer
        indexer_heads: Number of indexer heads
        indexer_dim: Dimension of indexer queries/keys
        dropout: Dropout probability
    """
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        max_seq_len: int,
        layer_idx: int,
        layer_top_k: int,
        indexer_heads: int = 4,
        indexer_dim: int = 64,
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.layer_idx = layer_idx
        self.layer_top_k = layer_top_k

        # Main attention components
        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        self.rotary = RotaryPositionalEmbeddings(dim=self.d_k, max_seq_len=max_seq_len, base=10000)
        self.dropout = dropout

        # Lightning indexer
        self.indexer = LightningIndexer(
            d_model=d_model,
            indexer_heads=indexer_heads,
            indexer_dim=indexer_dim,
            dropout=dropout
        )

        # Adaptive token selector
        self.selector = AdaptiveTopKSelector(default_top_k=layer_top_k)

        # Whether to use sparse attention
        self.use_sparse = True

    def forward(
        self,
        x: torch.Tensor,
        return_stats: bool = False
    ) -> Tuple[torch.Tensor, Optional[Dict]]:
        """
        Forward pass with adaptive sparse attention

        Args:
            x: Input tensor [batch_size, seq_len, d_model]
            return_stats: Whether to return selection statistics

        Returns:
            - output: Attention output [batch_size, seq_len, d_model]
            - stats: Selection statistics if return_stats=True
        """
        batch_size, seq_len, _ = x.shape

        # Compute Q, K, V
        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        Q, K, V = qkv[0], qkv[1], qkv[2]

        # Apply RoPE
        Q = self.rotary(Q.transpose(1, 2)).transpose(1, 2)
        K = self.rotary(K.transpose(1, 2)).transpose(1, 2)

        stats = None

        if self.use_sparse:
            # Compute index scores
            index_scores = self.indexer(x)

            # Select top-k tokens (using layer-specific k)
            top_k_mask, top_k_indices, selector_stats = self.selector(
                index_scores,
                top_k=self.layer_top_k,
                apply_causal_mask=True
            )

            # Create attention mask
            attn_mask = torch.zeros(
                batch_size, 1, seq_len, seq_len,
                device=x.device,
                dtype=Q.dtype
            )
            attn_mask = attn_mask.masked_fill(~top_k_mask.unsqueeze(1), float('-inf'))

            # Apply sparse attention
            attn_output = F.scaled_dot_product_attention(
                Q, K, V,
                attn_mask=attn_mask,
                dropout_p=self.dropout if self.training else 0.0
            )

            if return_stats:
                stats = {
                    'layer_idx': self.layer_idx,
                    'layer_k': self.layer_top_k,
                    **selector_stats
                }
        else:
            # Dense attention
            attn_output = F.scaled_dot_product_attention(
                Q, K, V,
                is_causal=True,
                dropout_p=self.dropout if self.training else 0.0
            )

            if return_stats:
                stats = {
                    'layer_idx': self.layer_idx,
                    'layer_k': seq_len,
                    'sparsity': 0.0,
                    'k_ratio': 1.0
                }

        # Reshape and project output
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)
        output = self.w_o(attn_output)

        return output, stats

    def enable_sparse(self):
        """Enable sparse attention"""
        self.use_sparse = True

    def disable_sparse(self):
        """Disable sparse attention (use dense)"""
        self.use_sparse = False

    def update_layer_k(self, new_k: int):
        """Update the layer's top-k value dynamically"""
        self.layer_top_k = new_k
        self.selector.default_top_k = new_k


def print_schedule_info(config: LayerSparsityConfig, n_layers: int):
    """Print detailed information about a sparsity schedule"""
    print(f"\n{'='*80}")
    print(f"Sparsity Schedule: {config.schedule_name}")
    print(f"{'='*80}")
    print(f"Description: {config.description}")
    print(f"\nPer-Layer Configuration:")
    print(f"{'Layer':<10} {'k Ratio':<15} {'Function':<30}")
    print(f"{'-'*80}")

    for i in range(n_layers):
        ratio = config.layer_k_ratios[i] if i < len(config.layer_k_ratios) else config.layer_k_ratios[-1]

        # Categorize layer
        early_cutoff = n_layers // 3
        middle_cutoff = 2 * n_layers // 3
        if i < early_cutoff:
            function = "Early (local patterns)"
        elif i < middle_cutoff:
            function = "Middle (feature composition)"
        else:
            function = "Late (global context)"

        print(f"Layer {i:<4} {ratio:<15.2%} {function:<30}")
    print(f"{'='*80}\n")


# ================= Qwen3-Next Components (Fallback) =================

import torch.nn.functional as F
import math

class Qwen3NextRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)


class Qwen3NextMLP(nn.Module):
    def __init__(self, config=None, intermediate_size=512, hidden_size=128):
        super().__init__()
        if config:
            hidden_size = getattr(config, 'hidden_size', 128)
            intermediate_size = intermediate_size or hidden_size * 4
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))


class Qwen3NextSparseMoeBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config.hidden_size
        self.num_experts = config.num_experts
        self.top_k = config.num_experts_per_tok
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)
        self.experts = nn.ModuleList([
            Qwen3NextMLP(intermediate_size=config.moe_intermediate_size, hidden_size=self.hidden_dim)
            for _ in range(self.num_experts)
        ])

    def forward(self, hidden_states):
        batch_size, seq_len, hidden_dim = hidden_states.shape
        hidden_states_flat = hidden_states.view(-1, hidden_dim)
        router_logits = self.gate(hidden_states_flat)
        routing_weights = F.softmax(router_logits, dim=1)
        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)

        final_hidden_states = torch.zeros_like(hidden_states_flat)
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            expert_mask = (selected_experts == expert_idx).any(dim=-1)
            if expert_mask.any():
                expert_input = hidden_states_flat[expert_mask]
                expert_output = expert_layer(expert_input)
                token_indices = expert_mask.nonzero(as_tuple=True)[0]
                expert_positions = (selected_experts[expert_mask] == expert_idx).nonzero(as_tuple=True)[1]
                weights = routing_weights[expert_mask, expert_positions].unsqueeze(-1)
                final_hidden_states[expert_mask] += expert_output * weights

        return final_hidden_states.view(batch_size, seq_len, hidden_dim)


class Qwen3NextRotaryEmbedding(nn.Module):
    """Fallback implementation of Qwen3NextRotaryEmbedding"""
    def __init__(self, config=None, dim=None, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        if config is not None:
            self.dim = config.head_dim if hasattr(config, 'head_dim') else config.hidden_size // config.num_attention_heads
            self.max_position_embeddings = config.max_position_embeddings
            self.base = config.rope_theta if hasattr(config, 'rope_theta') else 10000
        else:
            self.dim = dim
            self.max_position_embeddings = max_position_embeddings
            self.base = base

        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    @torch.no_grad()
    def forward(self, x, position_ids):
        # x: [batch_size, seq_len, ...]
        # position_ids: [batch_size, seq_len]
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        position_ids_expanded = position_ids[:, None, :].float()

        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos()
        sin = emb.sin()
        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


class Qwen3NextConfig:
    def __init__(self, **kwargs):
        for k, v in kwargs.items():
            setattr(self, k, v)


# ==================== EXACT Exp3 PLASAQwen3 Implementation ====================

class PLASADecoderLayer(nn.Module):
    """
    Decoder layer that uses Per-Layer Adaptive Sparse Attention for ALL attention
    (replaces both full_attention and linear_attention)

    EXACT COPY from exp3_models.py lines 49-110
    """
    def __init__(self, config, layer_idx: int, layer_top_k: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx

        # Use Per-Layer Adaptive Sparse Attention with layer-specific k
        self.self_attn = AdaptiveSparseAttention(
            d_model=config.hidden_size,
            n_heads=config.num_attention_heads,
            max_seq_len=config.max_position_embeddings,
            layer_idx=layer_idx,
            layer_top_k=layer_top_k,
            indexer_heads=getattr(config, 'indexer_heads', 4),
            indexer_dim=getattr(config, 'indexer_dim', 64),
            dropout=config.attention_dropout,
        )

        # MLP (same as Qwen3-Next)
        if (layer_idx not in config.mlp_only_layers) and (
            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0
        ):
            self.mlp = Qwen3NextSparseMoeBlock(config)
        else:
            self.mlp = Qwen3NextMLP(config, intermediate_size=config.intermediate_size)

        self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> torch.FloatTensor:
        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # DeepSeek Sparse Attention
        hidden_states, _ = self.self_attn(hidden_states)

        hidden_states = residual + hidden_states

        # MLP
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        if isinstance(hidden_states, tuple):
            hidden_states, _ = hidden_states
        hidden_states = residual + hidden_states

        return hidden_states


class PLASAQwen3Model(nn.Module):
    """
    Variant 2: All attention layers replaced with Per-Layer Adaptive Sparse Attention
    Uses PROGRESSIVE_SPARSE schedule: Early=Dense, Middle=L/4, Late=L/2

    EXACT COPY from exp3_models.py lines 202-277
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)

        # Create sparsity schedule for per-layer k values
        sparsity_config = create_sparsity_schedule(
            schedule=SparsitySchedule.PROGRESSIVE_SPARSE,
            n_layers=config.num_hidden_layers,
            seq_len=config.max_position_embeddings
        )

        # Replace all layers with PLASA decoder layers with layer-specific k values
        self.layers = nn.ModuleList([
            PLASADecoderLayer(
                config,
                layer_idx,
                layer_top_k=sparsity_config.get_k_for_layer(layer_idx, config.max_position_embeddings)
            )
            for layer_idx in range(config.num_hidden_layers)
        ])

        self.norm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3NextRotaryEmbedding(config=config)
        self.gradient_checkpointing = False

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ):
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )
        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        for decoder_layer in self.layers:
            hidden_states = decoder_layer(
                hidden_states,
                position_embeddings=position_embeddings,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                cache_position=cache_position,
                **kwargs,
            )

        hidden_states = self.norm(hidden_states)

        return type('ModelOutput', (), {
            'last_hidden_state': hidden_states,
            'past_key_values': past_key_values,
        })()


class PLASAQwen3(nn.Module):
    """
    Variant 2: Per-Layer Adaptive Sparse Attention (PLASA)-Only Qwen3 (for CausalLM)

    EXACT COPY from exp3_models.py lines 280-309
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.model = PLASAQwen3Model(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.LongTensor] = None,
        **kwargs,
    ):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        logits = self.lm_head(outputs.last_hidden_state)

        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        return type('CausalLMOutput', (), {
            'loss': loss,
            'logits': logits,
            'past_key_values': outputs.past_key_values,
        })()


# Create PLASAModel wrapper for benchmark compatibility
class PLASAModel(nn.Module):
    """
    Wrapper for PLASAQwen3 that accepts evaluator-style parameters
    and converts them to a Qwen3NextConfig.
    """
    def __init__(
        self,
        vocab_size: int,
        hidden_size: int = 128,
        n_layers: int = 4,
        n_heads: int = 4,
        num_kv_heads: int = 2,
        head_dim: int = 32,
        intermediate_size: int = 512,
        max_seq_len: int = 128,
        dropout: float = 0.1,
        rms_norm_eps: float = 1e-6,
    ):
        super().__init__()

        # Create Qwen3NextConfig from evaluator parameters (matching exp3)
        config = Qwen3NextConfig(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_hidden_layers=n_layers,
            num_attention_heads=n_heads,
            num_key_value_heads=num_kv_heads,
            head_dim=head_dim,
            intermediate_size=intermediate_size,
            max_position_embeddings=512,  # exp3 uses 512, not max_seq_len!
            rope_theta=10000.0,
            attention_dropout=dropout,
            hidden_dropout_prob=dropout,  # exp3 uses this
            partial_rotary_factor=1.0,  # exp3 uses this
            rms_norm_eps=rms_norm_eps,
            pad_token_id=0,
            # MoE parameters (EXACT match with exp3)
            num_experts=4,
            num_local_experts=4,
            num_experts_per_tok=2,
            decoder_sparse_step=2,
            moe_intermediate_size=256,
            shared_expert_intermediate_size=0,
            mlp_only_layers=[],
            # PLASA parameters (EXACT match with exp3)
            indexer_heads=4,
            indexer_dim=32,  # exp3 uses 32, not 64!
        )

        # Set attention implementation (required for full_attention layers)
        config._attn_implementation = "eager"

        # Create the actual PLASA model
        self.model = PLASAQwen3(config)

    def forward(self, input_ids, labels=None, **kwargs):
        return self.model(input_ids=input_ids, labels=labels, **kwargs)


__all__ = ['PLASAModel', 'AdaptiveSparseAttention', 'SparsitySchedule', 'create_sparsity_schedule']

```

# Previous Performance Metrics: 
- score: 0.0112
- perplexity: 89.2253
- accuracy: 0.4952
- train_loss: 6.3318
- val_loss: 4.4912

# Areas Identified for Improvement: 
- Consider simplifying the code to improve readability and maintainability



# Program Evolution History
## Previous Attempts

### Attempt 1
- Changes: Unknown changes
- Performance: score: 0.0112, perplexity: 89.2253, accuracy: 0.4952, train_loss: 6.3318, val_loss: 4.4912
- Outcome: Improvement in all metrics

## Top Performing Programs




# Current Proposal
**Title:**  
Dynamic Per-Layer Sparse Calibration (DPSC) for PLASA: Adaptive k-Budgeting and Context-Aware Indexing for Lower Perplexity

**Abstract:**  
The current PLASA implementation achieves only marginal gains over dense baselines (validation perplexity ≈89, accuracy ≈49%, 1/ppx ≈0.0112), indicating residual inefficiencies in static sparsity budgets, coarse indexer dynamics, and limited adaptation to token-specific context. We propose Dynamic Per-Layer Sparse Calibration (DPSC), a refinement that retains the progressive sparsity principle and Lightning Indexer while introducing finer-grained, learnable mechanisms inspired by Dynamic Attention Masking (DAM) and DeepSeek’s two-stage selection. DPSC (1) augments each layer with a lightweight gating network that predicts adaptive \(k_t^{(l)}\) per token based on contextual entropy and attention footprint, allowing middle layers to further reduce redundancy without harming downstream coherence; (2) replaces fixed top-k thresholds with learned per-layer percentile estimators trained via a small auxiliary objective that encourages calibration to true attention mass, mimicking DAM’s context-aware masks; (3) transforms the Lightning Indexer into a dual-path indexer where an FP8 approximate scoring stage filters candidates before a refined FP16 re-ranking, reducing false positives and making sparse masks tighter; (4) regularizes indexer learning through initialization tied to scaled RoPE statistics and stochastic depth on selection heads to improve gradient flow. Training-level improvements include a cosine learning-rate warmup, adaptive dropout scaling per layer, and self-distillation from a dense-attention teacher on a cached WikiText-2 subset to stabilize early training. We will evaluate DPSC on the same 4-layer, 128-dim PLASA model training regime (1000 steps, WikiText-2) and measure improvements in validation perplexity, accuracy, and sparsity efficiency over the baseline. By blending dynamic scheduling, context-aw...(truncated)

# Task
Suggest improvements to the program that will lead to better performance on the specified metrics.

You MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:

<<<<<<< SEARCH
# Original code to find and replace (must match exactly)
=======
# New replacement code
>>>>>>> REPLACE

Example of valid diff format:
<<<<<<< SEARCH
for i in range(m):
    for j in range(p):
        for k in range(n):
            C[i, j] += A[i, k] * B[k, j]
=======
# Reorder loops for better memory access pattern
for i in range(m):
    for k in range(n):
        for j in range(p):
            C[i, j] += A[i, k] * B[k, j]
>>>>>>> REPLACE

You can suggest multiple changes. Each SEARCH section must exactly match code in the current program.
Be thoughtful about your changes and explain your reasoning thoroughly.

IMPORTANT: Do not rewrite the entire program - focus on targeted improvements.

2025-11-16 16:35:15,246 - evolve_agent.llm.openai - DEBUG - ====================================================================================================
2025-11-16 16:35:15,246 - evolve_agent.llm.openai - DEBUG - API response: To squeeze more performance out of PLASA without rewriting large swaths of the code, I focused on two micro-optimizations that improve both runtime and regularization:

1. **Regularize the Lightning Indexer.** Dropout was already declared in `LightningIndexer` but never used, so the indexer never benefited from the stabilization that dropout provides. Injecting the dropout into the query and weight projections keeps the indexer from overfitting to spurious attention scores and improves its generalization, which can lower perplexity when training with few steps.

2. **Cache the causal mask in `AdaptiveTopKSelector`.** Constructing a full `triang` mask every forward pass creates significant overhead during training. By caching the mask per `(seq_len_q, seq_len_k, device)` triple and reusing it, we avoid repeated allocations, reducing time spent on mask construction each step. This change makes the sparse selection path more lightweight without changing semantics.

Both adjustments are targeted, add minimal code, and directly support your goal of improving throughput/per_step performance.

```diff
<<<<<<< SEARCH
        # Compute indexer queries: [batch, seq_len, indexer_heads, indexer_dim]
        queries = self.q_proj(x).reshape(batch_size, seq_len, self.indexer_heads, self.indexer_dim)

        # Compute indexer keys: [batch, seq_len, indexer_dim]
        keys = self.k_proj(x)

        # Compute indexer weights: [batch, seq_len, indexer_heads]
        weights = self.w_proj(x)
=======
        # Compute indexer queries: [batch, seq_len, indexer_heads, indexer_dim]
        queries = self.dropout(self.q_proj(x)).reshape(batch_size, seq_len, self.indexer_heads, self.indexer_dim)

        # Compute indexer keys: [batch, seq_len, indexer_dim]
        keys = self.k_proj(x)

        # Compute indexer weights: [batch, seq_len, indexer_heads]
        weights = self.dropout(self.w_proj(x))
>>>>>>> REPLACE
```

```diff
<<<<<<< SEARCH
        self.default_top_k = default_top_k

    def forward(
=======
        self.default_top_k = default_top_k
        self._causal_mask_cache: Dict[Tuple[int, int, str, int], torch.Tensor] = {}

    def _get_causal_mask(
        self,
        seq_len_q: int,
        seq_len_k: int,
        device: torch.device,
    ) -> torch.Tensor:
        device_idx = device.index if device.index is not None else -1
        key = (seq_len_q, seq_len_k, device.type, device_idx)
        mask = self._causal_mask_cache.get(key)
        if mask is None:
            if len(self._causal_mask_cache) >= 32:
                self._causal_mask_cache.clear()
            mask = torch.triu(
                torch.ones(seq_len_q, seq_len_k, device=device),
                diagonal=1
            ).bool()
            self._causal_mask_cache[key] = mask
        return mask

    def forward(
>>>>>>> REPLACE
```

```diff
<<<<<<< SEARCH
        if apply_causal_mask:
            causal_mask = torch.triu(
                torch.ones(seq_len_q, seq_len_k, device=index_scores.device),
                diagonal=1
            ).bool()
            index_scores = index_scores.masked_fill(causal_mask.unsqueeze(0), -1e9)
=======
        if apply_causal_mask:
            causal_mask = self._get_causal_mask(seq_len_q, seq_len_k, index_scores.device)
            index_scores = index_scores.masked_fill(causal_mask.unsqueeze(0), -1e9)
>>>>>>> REPLACE
```

These changes improve indexer generalization and remove redundant tensor allocations, which should translate into better training stability and slightly faster forward passes—helpful when optimizing training for the specified metrics.
2025-11-16 16:35:15,246 - evolve_agent.llm.openai - DEBUG - ====================================================================================================
2025-11-16 16:35:15,247 - evolve_agent.controller - INFO - Diff is applied successfully! 
